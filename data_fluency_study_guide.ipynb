{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Fluency Study Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Model Evaluation\n",
    "\n",
    "**Why evaluate models?**\n",
    "Model evaluation is a crucial step in the machine learning workflow. It helps us to:\n",
    "- **Assess Performance on Unseen Data:** The primary goal is to understand how well our model will generalize to new, previously unseen data. A model that performs well on data it was trained on might not necessarily perform well in the real world.\n",
    "- **Compare Models:** Evaluation metrics provide a basis for comparing different models or different versions of the same model (e.g., with different hyperparameters). This allows us to select the best-performing one for our specific task.\n",
    "- **Build Trust and Confidence:** Quantifiable performance metrics help in building trust in the model's predictions and decisions, especially when deploying models in critical applications.\n",
    "- **Identify Weaknesses and Guide Improvement:** Evaluation can reveal where a model is struggling (e.g., with specific classes in classification or certain ranges in regression), guiding further development and refinement.\n",
    "\n",
    "**Difference between training and test set performance. Overfitting concept.**\n",
    "When we train a machine learning model, we typically use a dataset called the **training set**. The model learns patterns and relationships from this data. \n",
    "\n",
    "- **Training Performance:** This is how well the model performs on the same data it was trained on. It's possible for a model to memorize the training data, leading to very high (sometimes perfect) training performance.\n",
    "- **Test Performance:** To get a more realistic assessment of how the model will perform on new data, we use a separate dataset called the **test set**. This data is *not* used during training. Test performance indicates the model's ability to generalize.\n",
    "\n",
    "**Overfitting:** A common problem in machine learning is **overfitting**. This occurs when a model learns the training data *too well*, including its noise and specific idiosyncrasies. \n",
    "- An overfit model will show high performance on the training set but poor performance on the test set.\n",
    "- It essentially fails to generalize to new, unseen data because it has learned the noise in the training data as if it were a real signal.\n",
    "- Underfitting is the opposite: the model fails to capture the underlying patterns in the training data and performs poorly on both training and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation (CV)\n",
    "\n",
    "**Principle:**\n",
    "Cross-validation is a resampling technique used to evaluate machine learning models on a limited data sample. Instead of a single split into training and test sets (which can be sensitive to how the split is made), CV provides a more robust and reliable estimate of model performance on unseen data.\n",
    "\n",
    "**K-Fold Cross-Validation:**\n",
    "This is the most common type of cross-validation. The process is as follows:\n",
    "1.  **Shuffle:** The dataset is first shuffled randomly (optional but recommended).\n",
    "2.  **Split:** The dataset is split into *K* equal-sized (or nearly equal-sized) partitions called \"folds\". Common values for K are 5 or 10.\n",
    "3.  **Iterate:** For each fold `i` from 1 to K:\n",
    "    a.  Use fold `i` as the **test set** (or validation set).\n",
    "    b.  Use the remaining `K-1` folds as the **training set**.\n",
    "    c.  Train the model on the training set.\n",
    "    d.  Evaluate the model on the test set (fold `i`) and record the performance metric (e.g., accuracy, RMSE).\n",
    "4.  **Average:** After K iterations, you will have K performance scores. The final CV performance is the average of these K scores.\n",
    "\n",
    "```\n",
    "Conceptual K-Fold CV Diagram (K=5):\n",
    "\n",
    "Data: [--------------------]\n",
    "\n",
    "Fold 1: [TTTT] [ Vaild] [TTTT] [TTTT] [TTTT]  -> Score 1\n",
    "Fold 2: [TTTT] [ TTTT ] [Valid] [TTTT] [TTTT]  -> Score 2\n",
    "Fold 3: [TTTT] [ TTTT ] [TTTT] [Valid] [TTTT]  -> Score 3\n",
    "Fold 4: [TTTT] [ TTTT ] [TTTT] [TTTT] [Valid]  -> Score 4\n",
    "Fold 5: [Valid] [ TTTT ] [TTTT] [TTTT] [TTTT]  -> Score 5\n",
    "\n",
    "(TTTT = Training data part, Valid = Validation/Test data part for that iteration)\n",
    "Final Score = Average(Score 1, Score 2, Score 3, Score 4, Score 5)\n",
    "```\n",
    "\n",
    "**Stratified K-Fold Cross-Validation:**\n",
    "In classification problems, especially with **imbalanced datasets** (where some classes have far fewer samples than others), standard K-Fold CV might result in folds where some classes are over-represented or even entirely missing in either the training or test split for a particular fold. \n",
    "\n",
    "**Stratified K-Fold CV** addresses this by ensuring that each fold maintains approximately the same percentage of samples for each target class as in the complete dataset. \n",
    "- **Importance:** This is crucial for imbalanced classification because it ensures that the model is trained and evaluated on representative proportions of each class in every fold, leading to more reliable and meaningful performance metrics.\n",
    "\n",
    "**Benefits of Cross-Validation:**\n",
    "- **Reduces Variance of Performance Estimate:** The average score from K folds is a more stable and less biased estimate of the model's true performance on unseen data compared to a single train-test split.\n",
    "- **Better Use of Data:** Every data point gets to be in a test set exactly once and in a training set K-1 times. This is particularly useful when the amount of available data is limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Metrics\n",
    "\n",
    "Once a classification model is trained, we need metrics to assess its performance.\n",
    "\n",
    "**Confusion Matrix:**\n",
    "A confusion matrix is a table that summarizes the performance of a classification model by showing the counts of true positive, true negative, false positive, and false negative predictions.\n",
    "\n",
    "Conceptual Layout (for a binary classification problem):\n",
    "```\n",
    "                      Predicted Class\n",
    "                    -------------------\n",
    "Actual Class      |   Positive (1)  |   Negative (0)  |\n",
    "------------------|-----------------|-----------------|\n",
    "Positive (1)      | True Positive   | False Negative  |\n",
    "                  |      (TP)       |      (FN)       |\n",
    "------------------|-----------------|-----------------|\n",
    "Negative (0)      | False Positive  | True Negative   |\n",
    "                  |      (FP)       |      (TN)       |\n",
    "------------------|-----------------|-----------------|\n",
    "```\n",
    "- **True Positive (TP):** Actual is Positive, Predicted is Positive.\n",
    "- **True Negative (TN):** Actual is Negative, Predicted is Negative.\n",
    "- **False Positive (FP):** Actual is Negative, Predicted is Positive (Type I Error).\n",
    "- **False Negative (FN):** Actual is Positive, Predicted is Negative (Type II Error).\n",
    "\n",
    "**Accuracy:**\n",
    "- **Formula:** `(TP + TN) / (TP + TN + FP + FN)` or `(Number of Correct Predictions) / (Total Number of Predictions)`\n",
    "- **Interpretation:** The proportion of total predictions that the model got right.\n",
    "- **Pros:** Simple and intuitive.\n",
    "- **Cons:** Can be very misleading on imbalanced datasets. For example, if 99% of cases are negative, a model that always predicts negative will have 99% accuracy but will be useless for identifying positive cases.\n",
    "\n",
    "**Precision:**\n",
    "- **Formula:** `TP / (TP + FP)`\n",
    "- **Interpretation:** \"Of all instances the model predicted as positive, what proportion was actually positive?\" It measures the accuracy of the positive predictions.\n",
    "- **When to prioritize:** When the cost of a False Positive is high. Examples:\n",
    "    - Spam detection: You don't want to incorrectly classify a legitimate email as spam (FP).\n",
    "    - Search engine results: You want the top results to be highly relevant (high precision).\n",
    "\n",
    "**Recall (Sensitivity or True Positive Rate - TPR):**\n",
    "- **Formula:** `TP / (TP + FN)`\n",
    "- **Interpretation:** \"Of all actual positive instances, what proportion did the model correctly identify?\" It measures the model's ability to find all positive samples.\n",
    "- **When to prioritize:** When the cost of a False Negative is high. Examples:\n",
    "    - Medical diagnosis (e.g., cancer detection): You don't want to miss an actual positive case (FN).\n",
    "    - Fraud detection: Failing to detect a fraudulent transaction (FN) can be costly.\n",
    "\n",
    "**F1-Score:**\n",
    "- **Formula:** `2 * (Precision * Recall) / (Precision + Recall)`\n",
    "- **Interpretation:** The harmonic mean of Precision and Recall. It provides a single score that balances both concerns. It's high when both precision and recall are high.\n",
    "- **When to use:** Useful when you need a balance between Precision and Recall, especially if there's an uneven class distribution.\n",
    "\n",
    "**ROC Curve & AUC:**\n",
    "- **ROC (Receiver Operating Characteristic) Curve:**\n",
    "    - A plot of the True Positive Rate (Recall) against the False Positive Rate (FPR) at various classification thresholds. FPR = `FP / (FP + TN)`.\n",
    "    - The threshold is the value above which a prediction is considered positive. By varying this threshold, we get different pairs of (TPR, FPR) values, which form the ROC curve.\n",
    "    - A model with good separability will have an ROC curve that bows towards the top-left corner (high TPR, low FPR).\n",
    "    - A random classifier would have a diagonal line from (0,0) to (1,1).\n",
    "- **AUC (Area Under the ROC Curve):**\n",
    "    - **Interpretation:** AUC represents the probability that a randomly chosen positive instance will be ranked higher (by the model's prediction score) than a randomly chosen negative instance. It's a measure of the model's ability to distinguish between classes.\n",
    "    - **Values:**\n",
    "        - AUC = 1: Perfect classifier.\n",
    "        - AUC = 0.5: No discriminative ability (like random guessing).\n",
    "        - AUC < 0.5: Worse than random (often means predictions are reversed).\n",
    "        - Generally, 0.7-0.8 is acceptable, 0.8-0.9 is good, >0.9 is excellent.\n",
    "    - **Benefit:** AUC is threshold-independent and useful for imbalanced datasets as it summarizes performance across all thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Metrics\n",
    "\n",
    "For regression tasks, where the goal is to predict a continuous value, different metrics are used.\n",
    "\n",
    "**Mean Absolute Error (MAE):**\n",
    "- **Formula:** `(1/n) * Σ |y_true - y_pred|` where n is the number of samples.\n",
    "- **Interpretation:** The average of the absolute differences between the true values and the predicted values. It gives an idea of the magnitude of errors, in the original units of the target variable.\n",
    "- **Robustness to Outliers:** MAE is less sensitive to outliers than MSE because it doesn't square the errors.\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "- **Formula:** `(1/n) * Σ (y_true - y_pred)²`\n",
    "- **Interpretation:** The average of the squared differences between the true and predicted values. \n",
    "- **Penalizes Large Errors More:** Squaring the errors gives significantly more weight to larger errors. This can be good if large errors are particularly undesirable, but it also means MSE can be heavily influenced by outliers.\n",
    "- **Units:** The units are the square of the target variable's units, which can make it harder to interpret directly.\n",
    "\n",
    "**Root Mean Squared Error (RMSE):**\n",
    "- **Formula:** `sqrt(MSE)` or `sqrt((1/n) * Σ (y_true - y_pred)²) `\n",
    "- **Interpretation:** The square root of the MSE. It's one of the most popular regression metrics because its units are the same as the target variable, making it more interpretable than MSE.\n",
    "- Like MSE, it penalizes larger errors more heavily due to the squaring step.\n",
    "\n",
    "**R-squared (R² or Coefficient of Determination):**\n",
    "- **Formula:** `1 - (SS_res / SS_tot)`\n",
    "    - `SS_res` (Sum of Squares of Residuals): `Σ (y_true - y_pred)²`\n",
    "    - `SS_tot` (Total Sum of Squares): `Σ (y_true - y_mean)²` (where `y_mean` is the mean of true values)\n",
    "- **Interpretation:** Represents the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features). \n",
    "    - R² = 1: The model perfectly explains all the variability of the response data around its mean.\n",
    "    - R² = 0: The model explains none of the variability (it's no better than predicting the mean of the target for all inputs).\n",
    "    - R² < 0: The model is worse than predicting the mean (can happen with poor models or when evaluated on unseen data where the model generalizes badly).\n",
    "- **Caution:** R² can be misleading. Adding more features to a model will almost always increase R² (or leave it unchanged), even if the features are not actually useful. Adjusted R² is a modification that accounts for the number of predictors in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (Classification Focus):**\n",
    "\n",
    "You are building a model to predict fraudulent transactions. This is a highly imbalanced dataset where only 1% of transactions are fraudulent.\n",
    "\n",
    "a) Which evaluation metric(s) would be most appropriate and why? Explain the pitfalls of using accuracy in this scenario.\n",
    "b) Describe how you would use Stratified K-Fold cross-validation to evaluate your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (Regression Focus):**\n",
    "\n",
    "You have developed two regression models to predict house prices. After 5-fold cross-validation, you get the following average results:\n",
    "- Model A: RMSE = \\$25,000, R-squared = 0.75\n",
    "- Model B: RMSE = \\$30,000, R-squared = 0.85\n",
    "\n",
    "Which model would you likely choose and why? Discuss the trade-offs or further investigations you might consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (General CV):**\n",
    "\n",
    "Explain why evaluating a model solely on its performance on the training dataset can be misleading. How does cross-validation address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question 1:**\n",
    "\n",
    "**a) Appropriate Metrics for Imbalanced Fraud Detection:**\n",
    "\n",
    "In a highly imbalanced dataset like fraud detection (1% fraudulent), accuracy is a poor choice. If the model simply predicted \"not fraudulent\" for every transaction, it would achieve 99% accuracy. However, it would completely fail at its primary goal: identifying fraudulent transactions.\n",
    "\n",
    "More appropriate metrics include:\n",
    "-   **Precision:** `TP / (TP + FP)`. In fraud detection, this tells us: \"Of all transactions flagged as fraudulent, how many actually were?\" High precision is important to ensure that when an alert is raised, it's likely to be a real fraud case, minimizing wasted effort investigating false alarms.\n",
    "-   **Recall (Sensitivity):** `TP / (TP + FN)`. This tells us: \"Of all actual fraudulent transactions, how many did the model correctly identify?\" This is often very critical in fraud detection because failing to catch a fraudulent transaction (a False Negative) can be very costly.\n",
    "-   **F1-Score:** `2 * (Precision * Recall) / (Precision + Recall)`. Since there's usually a trade-off between precision and recall (e.g., being more aggressive in flagging fraud might increase recall but lower precision), the F1-score provides a balance between the two. It's useful when both false positives and false negatives are important to minimize.\n",
    "-   **Area Under the ROC Curve (AUC-ROC):** This metric evaluates the model's ability to distinguish between fraudulent and non-fraudulent transactions across all possible classification thresholds. It's particularly good for imbalanced datasets because it's not dependent on a specific threshold choice and summarizes overall separability.\n",
    "-   **Precision-Recall Curve (AUC-PR):** For highly imbalanced datasets, the Precision-Recall curve (and its area, AUC-PR) can be even more informative than AUC-ROC. This is because AUC-ROC includes True Negatives in its calculation (via FPR = FP / (FP+TN)), and in highly imbalanced scenarios, the large number of TNs can sometimes make AUC-ROC seem overly optimistic. AUC-PR focuses directly on the performance of the positive (minority) class.\n",
    "\n",
    "**Pitfalls of Accuracy:** As explained, accuracy would be misleading. A model achieving 99% accuracy by always predicting the majority class (non-fraudulent) would be useless for the task of detecting fraud.\n",
    "\n",
    "**b) Using Stratified K-Fold Cross-Validation:**\n",
    "\n",
    "Stratified K-Fold CV is essential here to ensure that each fold maintains the 1% proportion of fraudulent transactions in both its training and validation subsets. This prevents scenarios where some folds might have very few or even no fraudulent examples in their validation set, which would make the evaluation unreliable.\n",
    "\n",
    "The process would be:\n",
    "1.  **Partition Data:** Divide the dataset into K folds (e.g., K=5 or K=10).\n",
    "2.  **Stratification:** Ensure that in each fold, approximately 1% of the instances are fraudulent, and 99% are non-fraudulent.\n",
    "3.  **Iteration:** For each fold `i`:\n",
    "    a.  The `i`-th fold serves as the validation set.\n",
    "    b.  The remaining `K-1` folds are combined to form the training set.\n",
    "    c.  Train the fraud detection model on this training set.\n",
    "    d.  Evaluate the trained model on the `i`-th validation set using metrics like Precision, Recall, F1-score, and AUC-ROC (or AUC-PR).\n",
    "4.  **Aggregate Results:** Average the scores for each metric obtained from the K iterations. This average provides a more robust estimate of the model's performance on unseen data while respecting the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Python code for Q1\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.linear_model import LogisticRegression # Example model\n",
    "\n",
    "# Assume X contains features, y contains labels (0 for non-fraud, 1 for fraud)\n",
    "# This is placeholder data - in reality, X would have many more samples and features.\n",
    "X = np.random.rand(1000, 10) \n",
    "y = np.zeros(1000)\n",
    "y[:10] = 1 # Create imbalance: 10 fraudulent cases (1%)\n",
    "np.random.shuffle(y) # Shuffle to mix fraudulent cases\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Fraudulent transactions: {np.sum(y == 1)}\")\n",
    "print(f\"Non-fraudulent transactions: {np.sum(y == 0)}\")\n",
    "\n",
    "# Initialize StratifiedKFold\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Lists to store scores from each fold\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "roc_auc_scores = []\n",
    "pr_auc_scores = []\n",
    "\n",
    "model = LogisticRegression(solver='liblinear', class_weight='balanced') # class_weight='balanced' can help with imbalance\n",
    "\n",
    "print(f\"\\nStarting Stratified {n_splits}-Fold Cross-Validation...\")\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n",
    "    print(f\"  Fold {fold+1}/{n_splits}\")\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Check class distribution in this fold's validation set (conceptual)\n",
    "    # print(f\"    Validation set fraud count: {np.sum(y_val == 1)} out of {len(y_val)} samples\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions (probabilities for ROC AUC and PR AUC)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1] # Probabilities for the positive class\n",
    "    y_pred_binary = model.predict(X_val) # Binary predictions for precision, recall, f1\n",
    "    \n",
    "    # Calculate metrics (handle cases where a class might be missing in a fold if not stratified or very small data)\n",
    "    # For binary classification, ensure positive class is present in y_val for meaningful scores\n",
    "    if np.sum(y_val == 1) > 0:\n",
    "        precision_scores.append(precision_score(y_val, y_pred_binary, zero_division=0))\n",
    "        recall_scores.append(recall_score(y_val, y_pred_binary, zero_division=0))\n",
    "        f1_scores.append(f1_score(y_val, y_pred_binary, zero_division=0))\n",
    "        roc_auc_scores.append(roc_auc_score(y_val, y_pred_proba))\n",
    "        pr_auc_scores.append(average_precision_score(y_val, y_pred_proba))\n",
    "    else:\n",
    "        # Handle case where positive class is not in validation fold (should not happen with StratifiedKFold if overall data has it)\n",
    "        print(f\"    Warning: Fold {fold+1} validation set has no positive samples. Metrics might be skewed or NaN.\")\n",
    "\n",
    "# Average scores\n",
    "print(\"\\n--- Average Cross-Validation Metrics ---\")\n",
    "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"Average Recall:    {np.mean(recall_scores):.4f}\")\n",
    "print(f\"Average F1-Score:  {np.mean(f1_scores):.4f}\")\n",
    "print(f\"Average ROC AUC:   {np.mean(roc_auc_scores):.4f}\")\n",
    "print(f\"Average PR AUC:    {np.mean(pr_auc_scores):.4f}\")\n",
    "\n",
    "print(\"\\nNote: Actual values depend on the random data and model. This is illustrative.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question 2:**\n",
    "\n",
    "This question involves comparing two regression models based on their RMSE and R-squared values.\n",
    "\n",
    "-   **Model A:** RMSE = \\$25,000, R-squared = 0.75\n",
    "-   **Model B:** RMSE = \\$30,000, R-squared = 0.85\n",
    "\n",
    "**Which model to choose and why?**\n",
    "\n",
    "There's a conflict here: Model A has a lower RMSE (which is better, as it indicates smaller average prediction errors), but Model B has a higher R-squared (which is also generally better, indicating it explains more variance in the data).\n",
    "\n",
    "1.  **RMSE (Root Mean Squared Error):**\n",
    "    *   Model A's predictions are, on average, \\$25,000 away from the actual house prices.\n",
    "    *   Model B's predictions are, on average, \\$30,000 away from the actual house prices.\n",
    "    *   **Lower RMSE is preferred.** So, based purely on RMSE, Model A seems better as its predictions are closer to the true values.\n",
    "\n",
    "2.  **R-squared (R²):**\n",
    "    *   Model A explains 75% of the variance in house prices.\n",
    "    *   Model B explains 85% of the variance in house prices.\n",
    "    *   **Higher R-squared is generally preferred.** So, based purely on R-squared, Model B seems better as it explains a larger proportion of the variability in the target variable.\n",
    "\n",
    "**Decision and Discussion:**\n",
    "\n",
    "The choice isn't straightforward and depends on the specific goals and context:\n",
    "\n",
    "-   **If the primary goal is prediction accuracy in absolute dollar terms:** Model A (lower RMSE) would likely be preferred. An average error of \\$25,000 is better than \\$30,000. RMSE is directly interpretable in the units of the target variable (dollars, in this case), making it very practical for understanding the typical error magnitude.\n",
    "\n",
    "-   **If the primary goal is to have a model that explains more of the underlying patterns and variability in house prices:** Model B (higher R-squared) might seem appealing. However, R-squared can be influenced by the number of predictors and can sometimes be high even if the individual prediction errors are large.\n",
    "\n",
    "**Trade-offs and Further Investigations:**\n",
    "\n",
    "1.  **Context Matters:** What is an acceptable error for house price prediction in this market? Is a \\$5,000 difference in RMSE significant given the typical house prices? If house prices are typically in the millions, this difference might be less critical than if they are around \\$100,000.\n",
    "\n",
    "2.  **Examine Error Distributions:** Don't rely solely on average metrics. Plot the distribution of residuals (prediction errors) for both models. \n",
    "    *   Does one model have more extreme outliers, even if its average error is lower/higher?\n",
    "    *   Is the error consistent across the range of house prices, or does one model perform poorly for expensive/cheap houses?\n",
    "\n",
    "3.  **Impact of Outliers:** RMSE is more sensitive to outliers than MAE. It would be beneficial to also look at the Mean Absolute Error (MAE) for both models. If Model B's higher RMSE is due to a few very large errors (outliers) but its MAE is comparable to or better than Model A's, this might change the perspective.\n",
    "\n",
    "4.  **Model Complexity and Overfitting:** Is Model B significantly more complex than Model A? A higher R-squared on training/validation data could sometimes be a sign of overfitting if the model is too complex, though cross-validation helps mitigate this. If Model B has many more features, its R-squared might be inflated. Consider looking at Adjusted R-squared.\n",
    "\n",
    "5.  **Business Objective:** What is the ultimate use of the model? \n",
    "    *   If it's for automated valuation for a mortgage company, minimizing large errors (and thus RMSE) might be critical.\n",
    "    *   If it's for understanding general market trends, R-squared might provide more insight into explanatory power.\n",
    "\n",
    "**Conclusion on Choice:**\n",
    "Most practitioners would lean towards **Model A (lower RMSE)** if the primary goal is predictive accuracy in terms of actual error magnitude. While Model B explains more variance, an R-squared of 0.75 is still respectable, and a lower average error is usually more practically valuable. However, the decision should be made after considering the points above, especially by examining error distributions and considering the business context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question 3:**\n",
    "\n",
    "**Why evaluating on training data is misleading:**\n",
    "\n",
    "Evaluating a model solely on its performance on the **training dataset** can be highly misleading because the model has already \"seen\" this data during its learning process. The model's parameters are optimized to minimize errors or maximize performance on this specific set of examples. This can lead to several issues:\n",
    "\n",
    "1.  **Overfitting:** The most significant issue is **overfitting**. An overfit model learns not only the underlying patterns in the training data but also its noise and random fluctuations. \n",
    "    *   Such a model might achieve excellent (or even perfect) performance on the training data because it has essentially memorized it.\n",
    "    *   However, when presented with new, unseen data (which will have different noise characteristics), its performance will likely be much worse. It fails to generalize.\n",
    "\n",
    "2.  **Inflated Performance Estimates:** Performance metrics calculated on the training set (e.g., accuracy, R-squared) are often overly optimistic and do not reflect how the model will perform in a real-world scenario on data it hasn't encountered before.\n",
    "\n",
    "3.  **Poor Model Selection:** If you compare multiple models based only on their training set performance, you might choose a more complex model that overfits heavily over a simpler, better-generalizing model.\n",
    "\n",
    "**How cross-validation addresses this issue:**\n",
    "\n",
    "Cross-validation (CV) provides a more robust and realistic way to estimate a model's performance on unseen data. Here's how it addresses the problems of evaluating solely on training data:\n",
    "\n",
    "1.  **Systematic Use of Validation Sets:** In K-Fold CV, the data is repeatedly split into a training set and a validation set (or test set for that fold). The model is trained on the training portion and evaluated on the validation portion. Crucially, the validation data in each fold is *unseen* by the model during the training phase of that specific iteration.\n",
    "\n",
    "2.  **Averaging Performance:** By training and validating the model K times on different subsets of the data, CV provides K different performance estimates. Averaging these estimates gives a more stable and less biased indication of how the model is likely to perform on average on new data.\n",
    "\n",
    "3.  **Reduces Dependence on a Single Split:** A simple train-test split can be sensitive to how the split is made. A lucky split might give an overly optimistic view, while an unlucky one might be too pessimistic. CV mitigates this by using multiple different splits, ensuring that every data point gets a chance to be in a validation set.\n",
    "\n",
    "4.  **Better Indication of Generalization:** Because the model is consistently evaluated on data it wasn't trained on within each fold, the resulting average performance metric is a much better indicator of the model's ability to generalize to independent datasets.\n",
    "\n",
    "In essence, cross-validation simulates the process of testing the model on multiple independent datasets, even when only one dataset is available, thereby providing a more reliable assessment of its true predictive power and helping to detect overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Python code for Q3\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression # Example model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assume X contains features, y contains labels\n",
    "X = np.random.rand(100, 5)\n",
    "y = (np.random.rand(100) > 0.5).astype(int) # Binary classification\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Scenario 1: Evaluating ONLY on training data (Misleading)\n",
    "model.fit(X, y) # Train on the entire dataset\n",
    "y_pred_on_training_data = model.predict(X)\n",
    "training_accuracy = accuracy_score(y, y_pred_on_training_data)\n",
    "print(f\"--- Scenario 1: Evaluating on Training Data Only ---\")\n",
    "print(f\"Accuracy on training data: {training_accuracy:.4f} (This can be misleadingly high)\")\n",
    "\n",
    "# Scenario 2: Using a single Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred_on_test_data = model.predict(X_test)\n",
    "test_accuracy_single_split = accuracy_score(y_test, y_pred_on_test_data)\n",
    "print(f\"\\n--- Scenario 2: Single Train-Test Split ---\")\n",
    "print(f\"Accuracy on test data (single split): {test_accuracy_single_split:.4f}\")\n",
    "\n",
    "# Scenario 3: Using Cross-Validation\n",
    "# Re-initialize model for a fair comparison if it was already trained\n",
    "model_cv = LogisticRegression(solver='liblinear') \n",
    "cv_scores = cross_val_score(model_cv, X, y, cv=5, scoring='accuracy') # 5-fold CV\n",
    "print(f\"\\n--- Scenario 3: Cross-Validation ---\")\n",
    "print(f\"Accuracy scores from each fold: {cv_scores}\")\n",
    "print(f\"Average accuracy from cross-validation: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Standard deviation of accuracy from cross-validation: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "print(\"\\nCross-validation provides a more robust estimate of how the model will perform on unseen data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Distribution Analysis\n",
    "\n",
    "**Purpose:**\n",
    "Distribution analysis is the process of examining the way data points are spread out or clustered. It helps in understanding:\n",
    "- **Data Patterns:** Identifying common values, the range of data, and areas where data is concentrated or sparse.\n",
    "- **Variability:** Quantifying how much data points differ from each other.\n",
    "- **Central Tendencies:** Finding typical or central values that represent the dataset.\n",
    "\n",
    "**Importance:**\n",
    "- **Data Quality Checks:** Identifying unusual patterns, errors, or outliers that might need investigation (e.g., a sensor producing values outside its expected range).\n",
    "- **Modeling Assumptions:** Many statistical models and machine learning algorithms make assumptions about the distribution of the data (e.g., normality for linear regression). Distribution analysis helps verify these assumptions.\n",
    "- **Comparing Groups:** Understanding if different groups or segments in the data have different distributions (e.g., do users in group A have higher purchase amounts than users in group B?).\n",
    "- **Feature Engineering:** Guiding transformations of features to make them more suitable for modeling (e.g., log transformation for skewed data).\n",
    "- **Informing Business Decisions:** Providing insights into data characteristics that can influence strategy (e.g., understanding the distribution of customer ages to target marketing campaigns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing Distributions\n",
    "\n",
    "We describe distributions using measures of central tendency, spread, and shape.\n",
    "\n",
    "**Central Tendency:**\n",
    "These measures describe the center or typical value of a dataset.\n",
    "-   **Mean:** \n",
    "    -   **Formula:** Sum of all values / Number of values (`Σx / n`).\n",
    "    -   **Sensitivity to Outliers:** The mean is sensitive to extreme values (outliers) because all values contribute to its calculation. A single very large or very small value can significantly pull the mean.\n",
    "-   **Median:**\n",
    "    -   **Definition:** The middle value in a dataset that has been sorted. If there's an even number of values, it's the average of the two middle values.\n",
    "    -   **Robustness to Outliers:** The median is robust to outliers because it only depends on the value(s) in the middle, not the extreme values.\n",
    "-   **Mode:**\n",
    "    -   **Definition:** The value that appears most frequently in a dataset.\n",
    "    -   **Use Cases:** Can be used for both numerical and categorical data. Particularly useful for describing the most common category or discrete value.\n",
    "    -   **Potential for Multiple Modes:** A dataset can have one mode (unimodal), two modes (bimodal), or more (multimodal).\n",
    "\n",
    "**Spread/Dispersion:**\n",
    "These measures describe how spread out or varied the data points are.\n",
    "-   **Variance / Standard Deviation:**\n",
    "    -   **Variance Formula:** Average of the squared differences from the Mean (`Σ(x - μ)² / N` for population, `Σ(x - x̄)² / (n-1)` for sample).\n",
    "    -   **Standard Deviation Formula:** Square root of the Variance.\n",
    "    -   **Interpretation:** Standard deviation measures the average distance of data points from the mean. A small standard deviation indicates data points tend to be close to the mean; a large standard deviation indicates data points are spread out over a wider range.\n",
    "-   **Interquartile Range (IQR):**\n",
    "    -   **Definition:** The difference between the third quartile (Q3, 75th percentile) and the first quartile (Q1, 25th percentile). `IQR = Q3 - Q1`.\n",
    "    -   **Robustness to Outliers:** IQR is robust to outliers because it's based on the middle 50% of the data.\n",
    "-   **Range:**\n",
    "    -   **Definition:** The difference between the maximum and minimum values in the dataset (`Max - Min`).\n",
    "    -   **Sensitivity to Outliers:** The range is highly sensitive to outliers as it directly uses the extreme values.\n",
    "\n",
    "**Shape:**\n",
    "These characteristics describe the form of the distribution.\n",
    "-   **Skewness:** Measures the asymmetry of the distribution.\n",
    "    -   **Positive Skewness (Right-skewed):** The tail on the right side of the distribution is longer or fatter than the left side. The mean is typically greater than the median. (e.g., income distributions).\n",
    "    -   **Negative Skewness (Left-skewed):** The tail on the left side is longer or fatter than the right side. The mean is typically less than the median. (e.g., test scores where most students do well).\n",
    "    -   **Symmetric:** Skewness is close to 0 (e.g., normal distribution).\n",
    "-   **Kurtosis:** Measures the \"tailedness\" or peakedness of a distribution relative to a normal distribution.\n",
    "    -   **Leptokurtic (Positive Kurtosis):** Sharper peak and heavier/fatter tails. More outliers than a normal distribution.\n",
    "    -   **Platykurtic (Negative Kurtosis):** Flatter peak and lighter/thinner tails. Fewer outliers than a normal distribution.\n",
    "    -   **Mesokurtic:** Similar peakedness and tail weight to a normal distribution (kurtosis around 0 or 3 depending on the formula used, e.g., Fisher vs Pearson).\n",
    "-   **Modality:** Describes the number of peaks in the distribution.\n",
    "    -   **Unimodal:** One clear peak.\n",
    "    -   **Bimodal:** Two distinct peaks. This might indicate the presence of two different subgroups in the data.\n",
    "    -   **Multimodal:** More than two peaks.\n",
    "\n",
    "**Common Visualizations:**\n",
    "-   **Histograms:** Bar charts showing the frequency of data points falling into specified ranges (bins). They reveal the shape, central tendency, and spread of the data.\n",
    "-   **Box Plots (Box-and-Whisker Plots):** Display the five-number summary (Minimum, Q1, Median, Q3, Maximum) and potential outliers. Excellent for comparing distributions across groups and identifying skewness and outliers.\n",
    "-   **Density Plots (Kernel Density Estimates):** Smoothed versions of histograms that provide a continuous estimate of the probability density function. Good for visualizing the shape of the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Outliers (Conceptual)\n",
    "\n",
    "**Definition:**\n",
    "Outliers are data points that significantly deviate from the other observations in a dataset. They are unusually high or low values compared to the rest of the data.\n",
    "\n",
    "**Importance:**\n",
    "-   **Skew Statistical Measures:** Outliers can heavily influence measures like the mean and standard deviation, giving a misleading summary of the data.\n",
    "-   **Impact Model Performance:** They can disproportionately affect the training of machine learning models, leading to models that don't generalize well.\n",
    "-   **Indicate Data Quality Issues:** Outliers can sometimes be due to data entry errors, measurement errors, or other data collection problems.\n",
    "-   **Represent True Extreme Values:** Sometimes, outliers are not errors but represent genuine, albeit rare, occurrences in the data (e.g., a very large fraudulent transaction).\n",
    "\n",
    "**Methods (Conceptual Explanations):**\n",
    "\n",
    "1.  **IQR Rule:**\n",
    "    -   Calculate the first quartile (Q1) and the third quartile (Q3).\n",
    "    -   Calculate the Interquartile Range (IQR) = Q3 - Q1.\n",
    "    -   Define an outlier as any data point below `Q1 - 1.5 * IQR` or above `Q3 + 1.5 * IQR`.\n",
    "    -   This method is robust because it uses Q1, Q3, and IQR, which are not heavily influenced by extreme values.\n",
    "\n",
    "2.  **Z-score (Standard Score):**\n",
    "    -   The Z-score measures how many standard deviations a data point is from the mean of the dataset.\n",
    "    -   **Formula:** `Z = (x - μ) / σ` (where x is the data point, μ is the mean, σ is the standard deviation).\n",
    "    -   For data that is approximately normally distributed, data points with an absolute Z-score greater than a certain threshold (commonly 2.5, 3, or 3.5) are considered outliers.\n",
    "    -   **Caution:** This method assumes the data is somewhat normally distributed and is sensitive to outliers itself (as mean and std dev are affected by outliers).\n",
    "\n",
    "3.  **Visual Inspection:**\n",
    "    -   **Box Plots:** Clearly show points that fall outside the \"whiskers\" (which are often defined using the IQR rule, e.g., 1.5 * IQR).\n",
    "    -   **Histograms/Density Plots:** Can reveal isolated bars or bumps at the extremes of the distribution.\n",
    "    -   **Scatter Plots:** Can help identify points that deviate from the general pattern of relationship between two variables.\n",
    "\n",
    "**Handling Outliers (Briefly Mention):**\n",
    "The approach to handling outliers depends on their cause and the goals of the analysis:\n",
    "-   **Investigation:** Try to understand why the outlier occurred (data entry error, genuine extreme, sensor malfunction?).\n",
    "-   **Correction:** If an outlier is confirmed to be an error (e.g., a typo), correct it if possible.\n",
    "-   **Removal:** If the outlier is an error and cannot be corrected, or if it's deemed irrelevant to the analysis and heavily skews results, it might be removed. This should be done cautiously and documented.\n",
    "-   **Transformation:** Applying mathematical transformations (e.g., log, square root) can sometimes reduce the skewness caused by outliers and make the data more suitable for certain models.\n",
    "-   **Using Robust Statistical Methods:** Employ statistical techniques or models that are less sensitive to outliers (e.g., using median instead of mean, robust regression).\n",
    "-   **Winsorization:** This technique limits the influence of extreme values by replacing them with less extreme values. For example, all data points below the 5th percentile might be set to the 5th percentile value, and all data points above the 95th percentile might be set to the 95th percentile value. This differs from trimming (removing outliers) as it retains the data point but changes its value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bootstrap\n",
    "\n",
    "**Principle:**\n",
    "The bootstrap is a powerful and versatile resampling method used in statistics to make inferences about a population from a sample. It works by treating the observed sample as if it were the entire population and repeatedly drawing samples *with replacement* from it. Each of these new samples is called a \"bootstrap sample,\" and they are the same size as the original sample.\n",
    "\n",
    "By calculating a statistic of interest (e.g., mean, median, standard deviation, correlation coefficient, regression coefficient) on each bootstrap sample, we can create an empirical distribution of that statistic. This \"bootstrap distribution\" approximates the sampling distribution of the statistic, which is the distribution of that statistic we would expect to see if we could draw many samples from the true underlying population.\n",
    "\n",
    "**Process:**\n",
    "1.  **Choose the number of bootstrap samples (B):** This is typically a large number, often 1,000 to 10,000 or more, to ensure stability in the results.\n",
    "2.  **Generate Bootstrap Samples:** For `b` from 1 to `B`:\n",
    "    a.  Draw `N` data points *with replacement* from the original sample of size `N`. This forms one bootstrap sample. (Note: \"With replacement\" means that after a data point is selected, it's put back into the pool and can be selected again. Thus, a bootstrap sample might contain duplicate instances from the original sample and might omit some original instances.)\n",
    "3.  **Calculate Statistic:** For each bootstrap sample, calculate the statistic of interest (e.g., mean, median, difference in means between two groups if applicable).\n",
    "4.  **Form Bootstrap Distribution:** The collection of these `B` calculated statistics forms the bootstrap distribution. This distribution provides an empirical estimate of how the statistic varies due to sampling variability.\n",
    "\n",
    "**Application 1: Estimating Confidence Intervals (CI):**\n",
    "A confidence interval provides a range of plausible values for a population parameter. The bootstrap distribution can be used to estimate CIs.\n",
    "-   **Percentile Method:** This is a common and intuitive method. To construct a (1 - α) * 100% confidence interval (e.g., a 95% CI where α = 0.05):\n",
    "    1.  Sort the `B` bootstrap statistics in ascending order.\n",
    "    2.  The lower bound of the CI is the (α/2)-th percentile of the bootstrap distribution (e.g., the 2.5th percentile for a 95% CI).\n",
    "    3.  The upper bound of the CI is the (1 - α/2)-th percentile of the bootstrap distribution (e.g., the 97.5th percentile for a 95% CI).\n",
    "    -   For example, if we have 1000 bootstrap means, the 95% CI would be between the 25th smallest mean and the 975th smallest mean.\n",
    "\n",
    "**Application 2: Hypothesis Testing (e.g., for comparing two independent samples A and B):**\n",
    "The bootstrap can be used for hypothesis testing, such as determining if the difference between the means (or other statistics) of two groups A and B is statistically significant.\n",
    "\n",
    "1.  **State Hypotheses:**\n",
    "    -   **Null Hypothesis (H0):** There is no true difference in the statistic between the two populations (e.g., mean_A = mean_B, or mean_A - mean_B = 0).\n",
    "    -   **Alternative Hypothesis (H1):** There is a true difference (e.g., mean_A ≠ mean_B, or mean_A > mean_B, or mean_A < mean_B).\n",
    "\n",
    "2.  **Calculate Observed Statistic:** Compute the actual difference in the statistic from the original samples: `obs_diff = statistic(sample_A) - statistic(sample_B)`.\n",
    "\n",
    "3.  **Simulate under H0 (Permutation/Resampling Approach for Two Samples):**\n",
    "    To simulate the null hypothesis that the two samples come from the same underlying distribution (i.e., there's no difference):\n",
    "    a.  **Combine Data:** Pool all data points from sample A and sample B into a single combined dataset.\n",
    "    b.  **For each bootstrap iteration (b=1 to B):**\n",
    "        i.  Draw a resample of size `n_A` (original size of sample A) *with replacement* from the **combined dataset**. This is `resample_A_from_combined`.\n",
    "        ii. Draw a resample of size `n_B` (original size of sample B) *with replacement* from the **combined dataset**. This is `resample_B_from_combined`.\n",
    "        (Alternatively, for permutation tests, you would shuffle the combined data and then take the first `n_A` as resample_A and the next `n_B` as resample_B, without replacement within each split of the shuffled data. For bootstrap hypothesis testing, resampling from the shifted data or a combined pool is common.)\n",
    "    c.  **Calculate Bootstrap Difference:** Compute the difference in the statistic for these resamples: `bootstrap_diff = statistic(resample_A_from_combined) - statistic(resample_B_from_combined)`.\n",
    "\n",
    "4.  **Calculate P-value:**\n",
    "    The p-value is the proportion of bootstrap differences that are as extreme or more extreme than the `obs_diff`.\n",
    "    -   For a two-tailed test (H1: mean_A ≠ mean_B): `p_value = (count of |bootstrap_diff| >= |obs_diff|) / B`.\n",
    "    -   For a one-tailed test (e.g., H1: mean_A > mean_B): `p_value = (count of bootstrap_diff >= obs_diff) / B`.\n",
    "\n",
    "5.  **Make Decision:** If the p-value is less than a chosen significance level (alpha, e.g., 0.05), reject H0 in favor of H1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (Latency Comparison & Bootstrap):**\n",
    "\n",
    "You have latency measurements (in milliseconds) for two software versions, A and B:\n",
    "A: [100, 105, 110, 103, 107, 99, 101, 104]\n",
    "B: [98, 102, 100, 105, 95, 99, 101, 97]\n",
    "\n",
    "a) Calculate the observed difference in mean latencies.\n",
    "b) Describe step-by-step how you would use the bootstrap method to determine if there is a statistically significant difference in mean latencies between version A and version B. What is the null hypothesis you are testing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (Distribution Interpretation):**\n",
    "\n",
    "A dataset of user engagement times (minutes per day) has the following summary statistics:\n",
    "Mean = 25, Median = 18, Std Dev = 15, IQR = 12, Skewness = 1.8, Min = 1, Max = 120.\n",
    "A data scientist also mentions that a box plot shows several points above the upper whisker.\n",
    "\n",
    "a) What can you infer about the shape of this distribution?\n",
    "b) What do the mean and median values tell you about the data?\n",
    "c) What does the presence of points above the upper whisker in a box plot suggest, and how might you conceptually use the IQR rule here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3 (Outlier Handling Philosophy):**\n",
    "\n",
    "When analyzing a dataset of customer purchase amounts, you detect several unusually high values using the Z-score method. What are the potential next steps you would consider before deciding to remove or modify these outliers? Why is it important not to automatically discard them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question 1:**\n",
    "\n",
    "**a) Calculate the observed difference in mean latencies:**\n",
    "\n",
    "Data:\n",
    "A: [100, 105, 110, 103, 107, 99, 101, 104]\n",
    "B: [98, 102, 100, 105, 95, 99, 101, 97]\n",
    "\n",
    "Mean of A (μA) = (100+105+110+103+107+99+101+104) / 8 = 829 / 8 = 103.625 ms\n",
    "Mean of B (μB) = (98+102+100+105+95+99+101+97) / 8 = 797 / 8 = 99.625 ms\n",
    "\n",
    "Observed difference in means (μA - μB) = 103.625 - 99.625 = **4.0 ms**\n",
    "\n",
    "**b) Using Bootstrap to Determine Statistical Significance:**\n",
    "\n",
    "**Null Hypothesis (H0):** There is no statistically significant difference in mean latencies between version A and version B. (i.e., μA - μB = 0, or μA = μB). The observed difference of 4.0 ms is due to random chance or sampling variability.\n",
    "\n",
    "**Alternative Hypothesis (H1):** There is a statistically significant difference in mean latencies (i.e., μA - μB ≠ 0). (We could also do a one-sided test, e.g., μA > μB, if we had a prior reason to expect A to be slower).\n",
    "\n",
    "**Step-by-Step Bootstrap Procedure for Hypothesis Testing:**\n",
    "\n",
    "1.  **Combine Data:** Pool all latency measurements from A and B into a single combined dataset:\n",
    "    `Combined = [100, 105, 110, 103, 107, 99, 101, 104, 98, 102, 100, 105, 95, 99, 101, 97]` (16 values)\n",
    "\n",
    "2.  **Set Number of Bootstrap Iterations (B):** Choose a large number, e.g., B = 10,000.\n",
    "\n",
    "3.  **Bootstrap Resampling and Calculation of Differences:** For each iteration `i` from 1 to B:\n",
    "    a.  **Create Bootstrap Sample A':** Draw 8 samples *with replacement* from the `Combined` dataset.\n",
    "    b.  **Create Bootstrap Sample B':** Draw 8 samples *with replacement* from the `Combined` dataset.\n",
    "    c.  **Calculate Means:** Compute the mean of Bootstrap Sample A' (μA') and the mean of Bootstrap Sample B' (μB').\n",
    "    d.  **Calculate Bootstrap Difference:** Store the difference: `diff_i = μA' - μB'`. This simulates the difference we might see if H0 were true (i.e., both samples came from the same underlying distribution).\n",
    "\n",
    "4.  **Form Bootstrap Distribution of Differences:** After B iterations, you will have a collection of `B` differences (`diff_1, diff_2, ..., diff_B`). This is your empirical bootstrap distribution of mean differences under the null hypothesis.\n",
    "\n",
    "5.  **Calculate P-value:** The p-value is the proportion of bootstrap differences that are as extreme or more extreme than the originally observed difference (4.0 ms).\n",
    "    -   For a two-tailed test (H1: μA ≠ μB), count how many `|diff_i| >= |observed_difference|` (i.e., `|diff_i| >= 4.0`).\n",
    "    -   `p_value = (Number of |diff_i| >= 4.0) / B`\n",
    "\n",
    "6.  **Make a Decision:**\n",
    "    -   If the p-value is less than a pre-determined significance level (alpha, e.g., α = 0.05), you reject the null hypothesis (H0). This would suggest that the observed difference of 4.0 ms is statistically significant and not just due to random chance.\n",
    "    -   If the p-value is greater than or equal to alpha, you fail to reject the null hypothesis. This would suggest that there isn't enough evidence to conclude a statistically significant difference in mean latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Python code for Q1\n",
    "import numpy as np\n",
    "\n",
    "A = np.array([100, 105, 110, 103, 107, 99, 101, 104])\n",
    "B_data = np.array([98, 102, 100, 105, 95, 99, 101, 97]) # Renamed to B_data to avoid conflict with num_bootstrap_samples\n",
    "\n",
    "# a) Calculate the observed difference in mean latencies\n",
    "mean_A = np.mean(A)\n",
    "mean_B = np.mean(B_data)\n",
    "observed_difference = mean_A - mean_B\n",
    "print(f\"Mean Latency A: {mean_A:.3f} ms\")\n",
    "print(f\"Mean Latency B: {mean_B:.3f} ms\")\n",
    "print(f\"Observed Difference (A - B): {observed_difference:.3f} ms\")\n",
    "\n",
    "# b) Bootstrap hypothesis testing\n",
    "print(\"\\n--- Bootstrap Hypothesis Test ---  \")\n",
    "combined_data = np.concatenate((A, B_data))\n",
    "n_A = len(A)\n",
    "n_B = len(B_data)\n",
    "num_bootstrap_samples = 10000  # B\n",
    "bootstrap_differences = []\n",
    "\n",
    "for _ in range(num_bootstrap_samples):\n",
    "    # Create bootstrap samples by resampling from the combined data\n",
    "    resample_A = np.random.choice(combined_data, size=n_A, replace=True)\n",
    "    resample_B = np.random.choice(combined_data, size=n_B, replace=True)\n",
    "    \n",
    "    # Calculate the difference in means for this bootstrap iteration\n",
    "    diff = np.mean(resample_A) - np.mean(resample_B)\n",
    "    bootstrap_differences.append(diff)\n",
    "\n",
    "# Calculate p-value (two-tailed test)\n",
    "# Proportion of bootstrap differences as extreme or more extreme than the observed difference\n",
    "p_value = np.sum(np.abs(bootstrap_differences) >= np.abs(observed_difference)) / num_bootstrap_samples\n",
    "\n",
    "print(f\"Number of bootstrap samples (B): {num_bootstrap_samples}\")\n",
    "print(f\"P-value from bootstrap test: {p_value:.4f}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(f\"Since p-value ({p_value:.4f}) < alpha ({alpha}), we reject the null hypothesis.\")\n",
    "    print(\"There is a statistically significant difference in mean latencies.\")\n",
    "else:\n",
    "    print(f\"Since p-value ({p_value:.4f}) >= alpha ({alpha}), we fail to reject the null hypothesis.\")\n",
    "    print(\"There is not enough evidence for a statistically significant difference in mean latencies.\")\n",
    "\n",
    "print(\"\\nNote: The exact p-value will vary slightly due to the random nature of bootstrapping.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question 2:**\n",
    "\n",
    "Summary statistics for user engagement times (minutes per day):\n",
    "- Mean = 25\n",
    "- Median = 18\n",
    "- Std Dev = 15\n",
    "- IQR = 12\n",
    "- Skewness = 1.8\n",
    "- Min = 1, Max = 120\n",
    "- Box plot shows several points above the upper whisker.\n",
    "\n",
    "**a) What can you infer about the shape of this distribution?**\n",
    "\n",
    "Several indicators point to the shape:\n",
    "1.  **Skewness:** A skewness value of 1.8 is positive and relatively high. This indicates the distribution is **positively skewed (right-skewed)**. This means the tail of the distribution extends further to the right (higher engagement times), and there's a concentration of data points at the lower end.\n",
    "2.  **Mean vs. Median:** The mean (25) is greater than the median (18). In a positively skewed distribution, the mean is pulled towards the longer tail (higher values), while the median is less affected. This relationship (Mean > Median) is characteristic of a right-skewed distribution.\n",
    "3.  **Max Value and Std Dev:** The maximum value (120) is quite far from the mean (25), and the standard deviation (15) is relatively large compared to the median, suggesting a wide spread, particularly towards the higher end, which is consistent with right skewness.\n",
    "4.  **Box Plot Points:** Points above the upper whisker in a box plot represent potential outliers on the higher end. This also supports the idea of a right-skewed distribution with some users having very high engagement times.\n",
    "\n",
    "**Conclusion on Shape:** The distribution is positively skewed (right-skewed), meaning most users have lower engagement times, but a smaller number of users have significantly higher engagement times, pulling the tail to the right.\n",
    "\n",
    "**b) What do the mean and median values tell you about the data?**\n",
    "\n",
    "-   **Median (18 minutes):** This is the middle value. 50% of users have engagement times of 18 minutes or less, and 50% have 18 minutes or more. In a skewed distribution, the median is often a better measure of central tendency or a \"typical\" value for the majority of users because it's not heavily influenced by extreme values.\n",
    "-   **Mean (25 minutes):** This is the arithmetic average engagement time. The fact that the mean (25) is higher than the median (18) is because the mean is pulled upwards by the users with very high engagement times (the right tail of the distribution).\n",
    "\n",
    "The difference between the mean and median reinforces the conclusion of positive skewness. It suggests that while a typical user (represented by the median) engages for around 18 minutes, a few highly engaged users significantly increase the overall average engagement time.\n",
    "\n",
    "**c) What does the presence of points above the upper whisker in a box plot suggest, and how might you conceptually use the IQR rule here?**\n",
    "\n",
    "-   **Suggestion from Box Plot:** Points above the upper whisker in a box plot are typically considered **potential outliers**. These are data points that are unusually high compared to the rest of the data, falling outside the main body of the distribution as captured by the box and its whiskers.\n",
    "\n",
    "-   **Conceptual Use of IQR Rule for Outlier Detection:**\n",
    "    The IQR rule is often used to define the whiskers of a box plot and identify outliers.\n",
    "    1.  **Given IQR = 12.**\n",
    "    2.  **Need Q1 and Q3:** Although not directly given, we know `IQR = Q3 - Q1`. To apply the rule, we would first need to calculate Q1 (the 25th percentile) and Q3 (the 75th percentile) from the raw data.\n",
    "    3.  **Calculate Outlier Thresholds:**\n",
    "        -   Lower Bound = `Q1 - 1.5 * IQR`\n",
    "        -   Upper Bound = `Q3 + 1.5 * IQR`\n",
    "    4.  **Identify Outliers:** Any data point (engagement time) that falls below the Lower Bound or above the Upper Bound would be flagged as a potential outlier according to this rule. Given the positive skewness and points above the upper whisker, we would primarily be looking for outliers on the higher end (i.e., values > `Q3 + 1.5 * IQR`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Python code for Q2 (c) - IQR Rule\n",
    "import numpy as np\n",
    "\n",
    "# Given summary statistics\n",
    "iqr = 12\n",
    "median = 18\n",
    "# We need Q1 and Q3 to fully apply the rule. Let's assume some plausible values for illustration,\n",
    "# knowing that median is 18 and IQR is 12. For example:\n",
    "# If the distribution is somewhat symmetric around the median within the box, \n",
    "# then Q1 could be median - IQR/2 and Q3 could be median + IQR/2. This is a simplification.\n",
    "# A more realistic scenario would be Q1 and Q3 are not symmetric around median in a skewed distribution.\n",
    "# Let's assume Q1 = 12 and Q3 = 24 for this conceptual example (Q3-Q1 = 12, median 18 is between them)\n",
    "Q1_example = 12 \n",
    "Q3_example = 24\n",
    "print(f\"Assuming for illustration: Q1 = {Q1_example}, Q3 = {Q3_example}, IQR = {Q3_example - Q1_example}\")\n",
    "\n",
    "# Calculate outlier thresholds using the IQR rule\n",
    "lower_bound = Q1_example - 1.5 * iqr\n",
    "upper_bound = Q3_example + 1.5 * iqr\n",
    "\n",
    "print(f\"Conceptual Lower Bound for Outliers: {lower_bound}\")\n",
    "print(f\"Conceptual Upper Bound for Outliers: {upper_bound}\")\n",
    "\n",
    "# Example data points (conceptual)\n",
    "engagement_times = np.array([1, 5, 10, 15, 18, 20, 22, 24, 30, 45, 50, 110, 120])\n",
    "print(f\"\\nExample Engagement Times: {engagement_times}\")\n",
    "\n",
    "potential_outliers = engagement_times[(engagement_times < lower_bound) | (engagement_times > upper_bound)]\n",
    "print(f\"Potential outliers based on assumed Q1/Q3 and IQR rule: {potential_outliers}\")\n",
    "\n",
    "print(\"\\nNote: Actual Q1 and Q3 would be calculated from the raw dataset.\")\n",
    "print(\"The problem states 'points above the upper whisker', so we'd focus on values > upper_bound.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question 3:**\n",
    "\n",
    "When unusually high purchase amounts are detected using the Z-score method, it's crucial not to automatically discard them. These values could be legitimate large purchases, data entry errors, or even fraudulent activities. Hasty removal can lead to loss of valuable information or biased analysis.\n",
    "\n",
    "Here are potential next steps to consider:\n",
    "\n",
    "1.  **Data Verification and Understanding Context:**\n",
    "    *   **Check for Data Entry Errors:** Are these values plausible? Could there have been a typo (e.g., extra zeros added, misplaced decimal point)? If possible, trace back to the source of data entry.\n",
    "    *   **Understand Business Context:** Are there known scenarios where such high purchase amounts are possible? (e.g., bulk orders, luxury items, B2B transactions vs. B2C, specific promotions or customer segments).\n",
    "    *   **Examine Individual Records:** Look at the full records associated with these high values. Do other features of these transactions make sense (e.g., type of product, customer history, location)?\n",
    "\n",
    "2.  **Assess the Nature of the Outliers:**\n",
    "    *   **Are they errors?** If confirmed as errors (e.g., a purchase amount of \\$1,000,000 for a \\$10 item), they should be corrected if possible, or removed if not correctable and clearly wrong.\n",
    "    *   **Are they genuine but rare events?** These could be your most valuable customers or specific types of transactions that are important to understand (e.g., a whale spender, a large corporate account).\n",
    "    *   **Are they indicative of specific phenomena?** For instance, in financial transactions, very high values might be flagged for fraud review.\n",
    "\n",
    "3.  **Consider the Goal of the Analysis:**\n",
    "    *   **Descriptive Statistics:** If the goal is to report a \"typical\" purchase amount, you might report the median (which is robust to outliers) alongside the mean. You could also report statistics with and without the outliers to show their impact.\n",
    "    *   **Predictive Modeling:** Outliers can disproportionately influence some models (e.g., linear regression, which minimizes squared errors). \n",
    "        *   You might consider transforming the data (e.g., log transformation) to reduce the impact of extreme values.\n",
    "        *   Using models that are inherently more robust to outliers (e.g., tree-based models like Random Forests, Gradient Boosting often handle outliers well; robust regression models).\n",
    "        *   Consider if the outliers represent a different underlying process that might need a separate model or specific features.\n",
    "    *   **Anomaly Detection:** If the goal is to find unusual behavior (like fraud), these outliers are precisely what you're looking for and should be investigated, not removed.\n",
    "\n",
    "4.  **Alternative Outlier Detection Methods:**\n",
    "    *   Don't rely solely on Z-scores, especially if the data isn't normally distributed. Use other methods like the IQR rule or visual inspection (box plots, histograms) to confirm if these values are indeed anomalous relative to the bulk of the data.\n",
    "\n",
    "5.  **Consider Modifying Instead of Removing (If Appropriate):**\n",
    "    *   **Winsorization:** As mentioned, this involves capping extreme values at a certain percentile (e.g., replacing all values above the 99th percentile with the 99th percentile value). This reduces their influence without completely removing them.\n",
    "    *   **Binning:** Grouping purchase amounts into categories (e.g., low, medium, high, very high) can sometimes mitigate the direct impact of the exact value of an outlier.\n",
    "\n",
    "**Why it's important not to automatically discard outliers:**\n",
    "\n",
    "-   **Loss of Information:** Outliers can contain valuable information about the data or the process generating it. A very high purchase amount could signify a VIP customer, a successful marketing campaign, or a new market segment. Discarding it means losing this insight.\n",
    "-   **Biased Results:** Removing data points selectively can bias your statistical analyses and model performance, making them unrepresentative of the true underlying data distribution or process.\n",
    "-   **Misleading Conclusions:** If outliers represent genuine but rare events, removing them can lead to underestimation of variability or risk (e.g., underestimating potential losses in finance if extreme loss events are removed).\n",
    "-   **Failure to Identify Problems or Opportunities:** Outliers can be signals of data quality issues that need fixing or unique opportunities (like a new customer demographic) that could be capitalized upon.\n",
    "\n",
    "In summary, detected outliers should trigger an investigation, not an automatic deletion. The decision on how to handle them should be informed by domain knowledge, the goals of the analysis, and an understanding of the data's context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Multiple Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Multiple Hypothesis Testing\n",
    "\n",
    "**The Problem:**\n",
    "In many analytical scenarios (e.g., A/B testing multiple features, genomic studies, fMRI brain mapping), we conduct not just one, but many statistical hypothesis tests simultaneously. When this happens, the probability of making at least one **Type I error** (a false positive – incorrectly rejecting a true null hypothesis) across the entire set of tests increases significantly.\n",
    "\n",
    "-   **Example:** If you perform a single hypothesis test with a significance level (alpha) of 0.05, you accept a 5% chance of making a Type I error if the null hypothesis is true.\n",
    "-   If you test 20 independent hypotheses, each at alpha = 0.05, and all null hypotheses are true, the probability of observing at least one false positive is `1 - (1 - 0.05)^20 = 1 - (0.95)^20 ≈ 1 - 0.358 = 0.642`.\n",
    "-   This means you have a ~64% chance of incorrectly concluding at least one test is significant, purely by chance!\n",
    "\n",
    "**Family-Wise Error Rate (FWER):**\n",
    "The **Family-Wise Error Rate (FWER)** is the probability of making **one or more** Type I errors when performing a set (or family) of multiple hypothesis tests.\n",
    "-   The goal of many multiple testing correction procedures is to control the FWER at a specified level (e.g., ≤ 0.05)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni Correction\n",
    "\n",
    "**Goal:**\n",
    "The Bonferroni correction is one of the simplest and most widely known methods to control the Family-Wise Error Rate (FWER).\n",
    "\n",
    "**Principle:**\n",
    "It adjusts the significance level (alpha) for each individual test to be more stringent, or equivalently, it adjusts the p-values of each test upwards, making it harder for any single test to be declared significant.\n",
    "\n",
    "**Method 1 (Adjusting Alpha):**\n",
    "1.  Decide on the desired overall FWER (e.g., alpha = 0.05).\n",
    "2.  Count the number of hypotheses being tested, `m`.\n",
    "3.  The significance level for each individual test (`alpha_corrected`) becomes: `alpha_corrected = alpha / m`.\n",
    "4.  An individual hypothesis test is considered statistically significant only if its p-value is less than or equal to `alpha_corrected` (i.e., `p <= alpha / m`).\n",
    "\n",
    "**Method 2 (Adjusting p-values):**\n",
    "1.  For each individual test `i`, obtain its p-value, `p_i`.\n",
    "2.  Multiply each p-value by the total number of hypotheses, `m`.\n",
    "    `p_adjusted_i = p_i * m`\n",
    "3.  Adjusted p-values are capped at 1.0 (since a probability cannot be greater than 1). So, `p_adjusted_i = min(p_i * m, 1.0)`.\n",
    "4.  Compare each `p_adjusted_i` to the original desired FWER (alpha, e.g., 0.05). If `p_adjusted_i <= alpha`, the test is considered significant.\n",
    "\n",
    "*(Note: Both methods are mathematically equivalent in terms of which hypotheses will be declared significant.)*\n",
    "\n",
    "**Pros:**\n",
    "-   **Simple:** Easy to understand and straightforward to implement.\n",
    "-   **Effective FWER Control:** It guarantees that the FWER will be less than or equal to the desired alpha level, under very general assumptions (it doesn't require tests to be independent).\n",
    "\n",
    "**Cons:**\n",
    "-   **Very Conservative:** The Bonferroni correction is often overly conservative, especially when the number of tests (`m`) is large. This means it significantly reduces the statistical power of the tests.\n",
    "-   **Increased Type II Errors:** By making it harder to declare any single test significant (to avoid false positives), it increases the probability of Type II errors (false negatives – failing to detect a true effect or difference when one actually exists).\n",
    "\n",
    "**When to Consider:**\n",
    "-   When the number of tests (`m`) is small.\n",
    "-   When it is critically important to avoid even a single false positive (e.g., in clinical trials where a false positive could lead to approving an ineffective drug, or in legal settings).\n",
    "-   As a quick, simple first pass, or when other methods are too complex to implement given constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1 (Applying Bonferroni):**\n",
    "\n",
    "A research team conducted 5 different A/B tests for new website features. The p-values obtained from these tests are:\n",
    "Test 1: p = 0.008\n",
    "Test 2: p = 0.025\n",
    "Test 3: p = 0.040\n",
    "Test 4: p = 0.150\n",
    "Test 5: p = 0.010\n",
    "\n",
    "The team wants to maintain an overall Family-Wise Error Rate (FWER) of 0.05.\n",
    "\n",
    "a) Why is it important to apply a correction in this scenario?\n",
    "b) Apply the Bonferroni correction to these p-values. Which tests remain statistically significant after the correction?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2 (Conceptual Understanding):**\n",
    "\n",
    "Explain the main trade-off involved when using the Bonferroni correction. In what kind of situation might its conservatism be a major drawback?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question 1:**\n",
    "\n",
    "**a) Why is it important to apply a correction in this scenario?**\n",
    "\n",
    "It's important to apply a correction because the team is conducting multiple (5) hypothesis tests simultaneously. If each test is evaluated at an individual alpha level of 0.05, the probability of making at least one Type I error (falsely rejecting a true null hypothesis, i.e., a false positive) across the entire family of 5 tests is greater than 0.05.\n",
    "\n",
    "Specifically, if all null hypotheses were true, the probability of at least one false positive would be:\n",
    "`1 - (1 - 0.05)^5 = 1 - (0.95)^5 ≈ 1 - 0.7738 = 0.2262`\n",
    "This means there would be a ~22.6% chance of incorrectly finding at least one feature significant when it's not. Applying a correction like Bonferroni aims to control this overall Family-Wise Error Rate (FWER) at the desired level of 0.05.\n",
    "\n",
    "**b) Apply the Bonferroni correction. Which tests remain statistically significant?**\n",
    "\n",
    "Number of tests (m) = 5\n",
    "Desired Family-Wise Error Rate (alpha_FWER) = 0.05\n",
    "\n",
    "**Method 1: Adjusting the significance level (alpha) for each test:**\n",
    "The corrected alpha for each individual test is `alpha_corrected = alpha_FWER / m = 0.05 / 5 = 0.01`.\n",
    "We compare each original p-value to this `alpha_corrected`:\n",
    "\n",
    "-   Test 1: p = 0.008. Since 0.008 ≤ 0.01, **Test 1 is significant.**\n",
    "-   Test 2: p = 0.025. Since 0.025 > 0.01, Test 2 is not significant.\n",
    "-   Test 3: p = 0.040. Since 0.040 > 0.01, Test 3 is not significant.\n",
    "-   Test 4: p = 0.150. Since 0.150 > 0.01, Test 4 is not significant.\n",
    "-   Test 5: p = 0.010. Since 0.010 ≤ 0.01, **Test 5 is significant.**\n",
    "\n",
    "**Method 2: Adjusting the p-values:**\n",
    "We multiply each p-value by `m = 5` and compare to the original `alpha_FWER = 0.05`.\n",
    "Adjusted p-value `p_adj = min(p_original * m, 1.0)`\n",
    "\n",
    "-   Test 1: `p_adj = min(0.008 * 5, 1.0) = min(0.04, 1.0) = 0.04`. Since 0.04 ≤ 0.05, **Test 1 is significant.**\n",
    "-   Test 2: `p_adj = min(0.025 * 5, 1.0) = min(0.125, 1.0) = 0.125`. Since 0.125 > 0.05, Test 2 is not significant.\n",
    "-   Test 3: `p_adj = min(0.040 * 5, 1.0) = min(0.200, 1.0) = 0.200`. Since 0.200 > 0.05, Test 3 is not significant.\n",
    "-   Test 4: `p_adj = min(0.150 * 5, 1.0) = min(0.750, 1.0) = 0.750`. Since 0.750 > 0.05, Test 4 is not significant.\n",
    "-   Test 5: `p_adj = min(0.010 * 5, 1.0) = min(0.050, 1.0) = 0.05`. Since 0.05 ≤ 0.05, **Test 5 is significant.**\n",
    "\n",
    "**Conclusion:**\n",
    "After applying the Bonferroni correction, **Test 1 and Test 5** remain statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Python code for Q1\n",
    "import numpy as np\n",
    "\n",
    "p_values = np.array([0.008, 0.025, 0.040, 0.150, 0.010])\n",
    "num_tests = len(p_values)\n",
    "alpha_fwer = 0.05\n",
    "\n",
    "print(f\"Original p-values: {p_values}\")\n",
    "print(f\"Number of tests (m): {num_tests}\")\n",
    "print(f\"Desired Family-Wise Error Rate (alpha_FWER): {alpha_fwer}\")\n",
    "\n",
    "# Method 1: Adjusting alpha\n",
    "bonferroni_alpha_corrected = alpha_fwer / num_tests\n",
    "print(f\"\\n--- Method 1: Adjusted Alpha ---\")\n",
    "print(f\"Bonferroni corrected alpha for individual tests: {bonferroni_alpha_corrected:.4f}\")\n",
    "significant_adj_alpha = p_values <= bonferroni_alpha_corrected\n",
    "print(f\"Significant tests (p <= corrected alpha): {significant_adj_alpha}\")\n",
    "for i, p_val in enumerate(p_values):\n",
    "    if p_val <= bonferroni_alpha_corrected:\n",
    "        print(f\"  Test {i+1} (p={p_val:.3f}) is significant.\")\n",
    "    else:\n",
    "        print(f\"  Test {i+1} (p={p_val:.3f}) is NOT significant.\")\n",
    "\n",
    "# Method 2: Adjusting p-values\n",
    "bonferroni_adjusted_p_values = np.minimum(p_values * num_tests, 1.0)\n",
    "print(f\"\\n--- Method 2: Adjusted P-values ---\")\n",
    "print(f\"Bonferroni adjusted p-values: {np.round(bonferroni_adjusted_p_values, 4)}\")\n",
    "significant_adj_p = bonferroni_adjusted_p_values <= alpha_fwer\n",
    "print(f\"Significant tests (adjusted p <= FWER alpha): {significant_adj_p}\")\n",
    "for i, adj_p_val in enumerate(bonferroni_adjusted_p_values):\n",
    "    if adj_p_val <= alpha_fwer:\n",
    "        print(f\"  Test {i+1} (original p={p_values[i]:.3f}, adjusted p={adj_p_val:.3f}) is significant.\")\n",
    "    else:\n",
    "        print(f\"  Test {i+1} (original p={p_values[i]:.3f}, adjusted p={adj_p_val:.3f}) is NOT significant.\")\n",
    "\n",
    "# Optional: Using statsmodels for more robust methods (though Bonferroni is simple enough manually)\n",
    "try:\n",
    "    from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "    reject, pvals_corrected, _, _ = multipletests(p_values, alpha=alpha_fwer, method='bonferroni')\n",
    "    print(\"\\n--- Using statsmodels.sandbox.stats.multicomp.multipletests (Bonferroni) ---\")\n",
    "    print(f\"Significant tests (reject null hypothesis): {reject}\")\n",
    "    print(f\"Corrected p-values: {np.round(pvals_corrected, 4)}\")\n",
    "except ImportError:\n",
    "    print(\"\\nStatsmodels library not found, skipping statsmodels example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer to Question 2:**\n",
    "\n",
    "**Main Trade-off of Bonferroni Correction:**\n",
    "\n",
    "The main trade-off when using the Bonferroni correction is between **controlling the Family-Wise Error Rate (FWER)** and **maintaining statistical power**.\n",
    "\n",
    "1.  **Controlling FWER (Pro):** The Bonferroni correction is very effective at reducing the probability of making one or more Type I errors (false positives) across a family of tests. By making the significance threshold for each individual test more stringent (`alpha / m`), it strongly guards against declaring a result significant when the null hypothesis is actually true.\n",
    "\n",
    "2.  **Reducing Statistical Power (Con):** This strict control comes at a cost: a reduction in statistical power. Power is the probability of correctly rejecting a null hypothesis when it is false (i.e., detecting a true effect). Because the Bonferroni correction makes the criterion for significance so much harder to meet for each individual test, it increases the chance of Type II errors (false negatives). This means you are more likely to miss true effects or real differences, especially if those effects are modest in size.\n",
    "\n",
    "In simpler terms, Bonferroni tries very hard not to call anything a discovery unless it's exceptionally strong, which means it might overlook genuine but less obvious discoveries.\n",
    "\n",
    "**Situation where its conservatism is a major drawback:**\n",
    "\n",
    "The conservatism of the Bonferroni correction can be a major drawback in **exploratory research** or **discovery-oriented studies**, especially when dealing with a **large number of hypotheses (`m` is large)**.\n",
    "\n",
    "Examples:\n",
    "-   **Genomics/Microarray Studies:** Researchers might test thousands or tens of thousands of genes simultaneously to see if their expression levels differ between two conditions (e.g., diseased vs. healthy tissue). If `m = 10,000` and the desired FWER is 0.05, the Bonferroni-corrected alpha for each gene would be `0.05 / 10,000 = 0.000005`. Only extremely small p-values would be considered significant. This could lead to missing many genes that genuinely have different expression levels but whose effects are not overwhelmingly strong.\n",
    "-   **Neuroimaging (fMRI):** When analyzing fMRI data, researchers might test for activation in tens of thousands of brain voxels. Applying Bonferroni would make it very difficult to find any activated regions unless the signal is incredibly strong.\n",
    "-   **Large-Scale A/B Testing in Web Analytics:** If a company tests hundreds of small website variations simultaneously, Bonferroni might prevent the detection of several small but genuinely positive improvements because each individual test's significance threshold becomes exceedingly low.\n",
    "\n",
    "In these scenarios, while controlling false positives is still important, the extreme stringency of Bonferroni can lead to an unacceptable number of false negatives, hindering scientific discovery or the identification of subtle but real effects. This is why other methods like the False Discovery Rate (FDR) controlling procedures (e.g., Benjamini-Hochberg) are often preferred in such exploratory, high-dimensional settings, as they offer a different balance between controlling false positives and maintaining power to make discoveries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
