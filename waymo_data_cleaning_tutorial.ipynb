{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Data Cleaning for Autonomous Vehicles: A Guide to the Waymo Open Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "**Importance of Data Quality in Autonomous Driving:**\n",
    "The safety and reliability of autonomous vehicles (AVs) are critically dependent on the quality of the data they use to perceive the world and make decisions. Raw sensor data from cameras, LiDAR, and RADAR is susceptible to noise, distortions, occlusions, and various artifacts. Effective data cleaning and quality enhancement are therefore indispensable steps in building robust AV systems.\n",
    "\n",
    "**Overview of the Waymo Open Dataset:**\n",
    "The Waymo Open Dataset is one of the largest and most diverse publicly available datasets for autonomous driving research. It contains high-resolution sensor data collected by Waymo's self-driving vehicles across various urban and suburban environments. The data is provided in the TFRecord format, and Waymo offers a TensorFlow-based Software Development Kit (SDK) for ease of use.\n",
    "Key features include:\n",
    "- Data from multiple high-resolution cameras.\n",
    "- Data from multiple LiDAR sensors, providing dense 3D point clouds.\n",
    "- Synchronization between sensor data.\n",
    "- Labeled data for 3D object detection and tracking.\n",
    "\n",
    "**Tutorial Goals and Conceptual Nature of Code:**\n",
    "This tutorial aims to provide a practical guide to common data cleaning and quality enhancement techniques applicable to the Waymo Open Dataset. We will explore:\n",
    "- How to conceptually parse and access sensor data using the Waymo Open Dataset SDK (TensorFlow-based).\n",
    "- Common cleaning techniques for camera and LiDAR data.\n",
    "- Strategies for ensuring multi-sensor consistency.\n",
    "The Python code provided in this notebook is **conceptual and illustrative**. It demonstrates how one *would* use the Waymo SDK and libraries like TensorFlow, NumPy, and Matplotlib. Direct execution of these snippets is not intended in this environment, as it would require the full dataset and SDK setup. The focus is on the *logic and patterns* involved in data cleaning for this specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setting Up (Conceptual)\n",
    "\n",
    "**Accessing the Waymo Open Dataset:**\n",
    "To work with the Waymo Open Dataset, you would typically:\n",
    "1.  Visit the [Waymo Open Dataset website](https://waymo.com/open/download/) and register to get access to the data.\n",
    "2.  Download the TFRecord files. These are large, so ensure you have sufficient storage and bandwidth.\n",
    "3.  Familiarize yourself with the dataset documentation, which details the data format, sensor specifications, and calibration.\n",
    "\n",
    "**Overview of the Waymo Open Dataset SDK (TensorFlow-based):**\n",
    "Waymo provides an official SDK, which includes TensorFlow operations for parsing the TFRecord files and helper functions for tasks like projection. You would typically install this SDK in your Python environment.\n",
    "```bash\n",
    "# Conceptual: How you might install the SDK\n",
    "# pip install waymo-open-dataset-tf-2-6-0 --user \n",
    "# (Replace with the latest compatible version)\n",
    "```\n",
    "\n",
    "**Key Libraries:**\n",
    "- **TensorFlow:** Essential for parsing TFRecords and using the Waymo SDK.\n",
    "- **NumPy:** For numerical operations on sensor data (e.g., point cloud manipulation).\n",
    "- **Matplotlib:** For visualizing images and point clouds (or other visualization tools like Open3D).\n",
    "- **OpenCV:** Useful for image processing tasks like undistortion, although some functionalities might be achievable with TensorFlow image operations too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual: Importing necessary libraries\n",
    "# import tensorflow as tf\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import cv2 # If using OpenCV for some operations\n",
    "\n",
    "# # Waymo SDK imports (actual paths might vary based on SDK version and installation)\n",
    "# from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "# from waymo_open_dataset.utils import frame_utils, transform_utils, range_image_utils\n",
    "\n",
    "print(\"Conceptual: Imported TensorFlow, NumPy, Matplotlib, and Waymo SDK components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Understanding Waymo Data Structures (TFRecords)\n",
    "\n",
    "**Explanation of TFRecords:**\n",
    "The Waymo Open Dataset is distributed as a set of TFRecord files. TFRecord is a TensorFlow-specific binary file format that is efficient for storing large amounts of sequential data. Each TFRecord file contains a sequence of serialized `Frame` protocol buffer messages.\n",
    "\n",
    "**`Frame` Protocol Buffers (`dataset_pb2.Frame`):**\n",
    "The core data structure is the `Frame`. Each `Frame` proto contains all sensor data (images, LiDAR points, poses) captured at a specific timestamp. Key fields within a `Frame` include:\n",
    "- `timestamp_micros`: Timestamp of the frame.\n",
    "- `images`: A list of `CameraImage` protos, one for each camera.\n",
    "- `lasers`: A list of `Laser` protos (e.g., TOP, FRONT, SIDE_LEFT, SIDE_RIGHT, REAR LiDARs).\n",
    "- `pose`: Vehicle pose at the time of frame capture.\n",
    "- `context`: Contains calibration data and other static information about the segment (sequence of frames).\n",
    "\n",
    "**Parsing `Frame`s using TensorFlow:**\n",
    "The Waymo SDK leverages TensorFlow's capabilities to read and parse these TFRecord files. You would typically create a `tf.data.TFRecordDataset` and then map a parsing function over it.\n",
    "\n",
    "**Extracting Basic Information:**\n",
    "Once a `Frame` is parsed, you can access its fields to get timestamps, check for the presence of data from different sensors, and retrieve sensor-specific information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Code: Parsing a Frame from a TFRecord file\n",
    "\n",
    "# # Assume 'segment_tfrecord_path' is the path to a Waymo TFRecord file\n",
    "# segment_tfrecord_path = '/path/to/waymo/dataset/segment-xxxxxxxx.tfrecord'\n",
    "\n",
    "# def parse_frame(frame_serialized):\n",
    "#     frame = open_dataset.Frame()\n",
    "#     frame.ParseFromString(bytearray(frame_serialized.numpy()))\n",
    "#     return frame\n",
    "\n",
    "# # Create a TensorFlow dataset\n",
    "# # try:\n",
    "# #     dataset = tf.data.TFRecordDataset(segment_tfrecord_path, compression_type='')\n",
    "# #     # Parse the first frame as an example\n",
    "# #     for serialized_frame_data in dataset.take(1):\n",
    "# #         frame = parse_frame(serialized_frame_data)\n",
    "# #         print(f\"Successfully parsed a Frame with timestamp: {frame.timestamp_micros}\")\n",
    "# #         print(f\"Context name: {frame.context.name}\")\n",
    "# #         print(f\"Number of camera images: {len(frame.images)}\")\n",
    "# #         print(f\"Number of laser scans: {len(frame.lasers)}\")\n",
    "\n",
    "# #         # Example: Accessing camera calibration for the first camera image\n",
    "# #         if len(frame.images) > 0:\n",
    "# #             camera_name = open_dataset.CameraName.Name.Name(frame.images[0].name)\n",
    "# #             for calib in frame.context.camera_calibrations:\n",
    "# #                 if calib.name == frame.images[0].name:\n",
    "# #                     print(f\"Calibration for camera {camera_name}: Intrinsics {calib.intrinsic}\")\n",
    "# #                     break\n",
    "# # except Exception as e:\n",
    "# #     print(f\"Error parsing TFRecord (conceptual): {e}. Ensure TensorFlow and SDK are set up and path is correct.\")\n",
    "\n",
    "print(\"Conceptual: Parsed a Frame from TFRecord and extracted basic info.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Camera Data Cleaning and Quality Enhancement\n",
    "\n",
    "Waymo provides data from multiple cameras around the vehicle. Ensuring image quality is vital for visual perception tasks.\n",
    "\n",
    "**a) Accessing Camera Data from `Frame`:**\n",
    "- Each `Frame` contains a list of `CameraImage` protos (`frame.images`).\n",
    "- `CameraImage` includes `image` (the byte string of the JPEG compressed image), `name` (e.g., FRONT, SIDE_LEFT), `pose` (camera pose relative to vehicle), and `velocity`.\n",
    "- Images need to be decoded (e.g., using `tf.image.decode_jpeg`).\n",
    "- **Why it's important:** Accessing the correct camera and decoding it properly is the first step before any cleaning can occur.\n",
    "\n",
    "**b) Image Undistortion:**\n",
    "- **Concept:** Correcting lens distortions (radial, tangential) to ensure geometric accuracy.\n",
    "- **Waymo Data:** Camera calibration (intrinsics, extrinsics, distortion parameters) is provided in `frame.context.camera_calibrations` for each camera name.\n",
    "- **Why it's important:** Crucial for accurate 3D understanding, feature matching, and projecting data from other sensors.\n",
    "- **Techniques (Conceptual):**\n",
    "    - Use `camera_calibrations[i].intrinsic` (f_x, f_y, c_x, c_y, k_1, k_2, k_3, p_1, p_2) and `camera_calibrations[i].extrinsic`.\n",
    "    - TensorFlow's `tf.raw_ops.CameraUndistortImage` or OpenCV's `cv2.undistort()` can be used with these parameters.\n",
    "\n",
    "**c) Handling Camera Artifacts:**\n",
    "- **Concept:** Issues like over/underexposure, lens flare, raindrops, dirt on the lens.\n",
    "- **Why it's important:** Artifacts can significantly degrade perception performance.\n",
    "- **Techniques (Conceptual):**\n",
    "    - **Exposure Check:** Analyze image histograms (e.g., using `tf.histogram_fixed_width` or `cv2.calcHist`). Flag images that are too dark or too bright.\n",
    "    - **Flare/Raindrop Detection:** More complex; might involve specific image processing algorithms or ML models. For this tutorial, we'll focus on simpler flagging based on brightness anomalies or manual review cues.\n",
    "    - **Waymo data quality:** Waymo's sensors are high quality, but extreme conditions can still cause artifacts.\n",
    "\n",
    "**d) Color/Lighting Normalization (Discussion):**\n",
    "- **Concept:** Adjusting images to have consistent color and brightness across different times of day and lighting conditions.\n",
    "- **Why it's important:** Improves robustness of algorithms that rely on color features.\n",
    "- **Techniques:** Histogram equalization (`tf.image.adjust_gamma`, `tf.image.equalize_histogram`), white balance algorithms. This can be complex and dataset-specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Code: Accessing and Undistorting a Camera Image\n",
    "\n",
    "# # Assume 'frame' is a parsed open_dataset.Frame object\n",
    "# # Assume 'camera_name_to_process' is an open_dataset.CameraName.Name enum, e.g., open_dataset.CameraName.FRONT\n",
    "\n",
    "# def get_camera_calibration(frame, camera_name_enum):\n",
    "#     for calib in frame.context.camera_calibrations:\n",
    "#         if calib.name == camera_name_enum:\n",
    "#             return calib\n",
    "#     return None\n",
    "\n",
    "# # Conceptual: Process the first camera image if available\n",
    "# # if frame and len(frame.images) > 0:\n",
    "# #     cam_image_proto = None\n",
    "# #     for img_proto in frame.images:\n",
    "# #         if img_proto.name == open_dataset.CameraName.FRONT: # Example: Process front camera\n",
    "# #             cam_image_proto = img_proto\n",
    "# #             break\n",
    "\n",
    "# #     if cam_image_proto:\n",
    "# #         # 1. Decode Image\n",
    "# #         # image_tensor = tf.image.decode_jpeg(cam_image_proto.image)\n",
    "# #         # print(f\"Decoded image tensor shape: {image_tensor.shape}\")\n",
    "\n",
    "# #         # # Display raw image (conceptual)\n",
    "# #         # # plt.figure(figsize=(10, 7))\n",
    "# #         # # plt.imshow(image_tensor.numpy())\n",
    "# #         # # plt.title(f\"Raw Image: {open_dataset.CameraName.Name.Name(cam_image_proto.name)}\")\n",
    "# #         # # plt.show()\n",
    "\n",
    "# #         # 2. Get Calibration\n",
    "# #         # camera_calib = get_camera_calibration(frame, cam_image_proto.name)\n",
    "# #         # if camera_calib:\n",
    "# #         #     intrinsics = np.array(camera_calib.intrinsic) # f_x, f_y, c_x, c_y, k_1, k_2, p_1, p_2, k_3\n",
    "# #         #     # Note: Waymo provides 9 distortion params (k1,k2,p1,p2,k3,k4,k5,k6), but OpenCV typically uses 5 (k1,k2,p1,p2,k3) or more.\n",
    "# #         #     # For OpenCV, you might use: K = np.array([[intrinsics[0], 0, intrinsics[2]], [0, intrinsics[1], intrinsics[3]], [0,0,1]])\n",
    "# #         #     # D = np.array(intrinsics[4:9]) # Or select appropriate ones for cv2.undistort\n",
    "\n",
    "# #         #     # Conceptual Undistortion (e.g. with OpenCV, if image_tensor converted to NumPy array)\n",
    "# #         #     # raw_image_np = image_tensor.numpy()\n",
    "# #         #     # K_cv = np.array([[intrinsics[0], 0, intrinsics[2]], [0, intrinsics[1], intrinsics[3]], [0,0,1]])\n",
    "# #         #     # D_cv = np.array([intrinsics[4], intrinsics[5], intrinsics[6], intrinsics[7], intrinsics[8]]) # k1,k2,p1,p2,k3 \n",
    "# #         #     # undistorted_image_np = cv2.undistort(raw_image_np, K_cv, D_cv)\n",
    "# #         #     # print(\"Image undistorted conceptually using OpenCV.\")\n",
    "\n",
    "# #         #     # # Display undistorted image (conceptual)\n",
    "# #         #     # # plt.figure(figsize=(10, 7))\n",
    "# #         #     # # plt.imshow(undistorted_image_np)\n",
    "# #         #     # # plt.title(f\"Undistorted Image: {open_dataset.CameraName.Name.Name(cam_image_proto.name)}\")\n",
    "# #         #     # # plt.show()\n",
    "# #         # else:\n",
    "# #         #     print(f\"Calibration not found for camera {open_dataset.CameraName.Name.Name(cam_image_proto.name)}.\")\n",
    "# #     else:\n",
    "# #         print(\"Selected camera image not found in frame.\")\n",
    "\n",
    "print(\"Conceptual: Accessed camera image, calibration, and performed undistortion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LiDAR Data Cleaning and Quality Enhancement\n",
    "\n",
    "Waymo's dataset includes data from multiple LiDAR sensors, providing rich 3D information.\n",
    "\n",
    "**a) Accessing LiDAR Data from `Frame`:**\n",
    "- `frame.lasers` contains a list of `Laser` protos.\n",
    "- Each `Laser` proto includes `name` (e.g., TOP, REAR) and `ri_return1` (first return), `ri_return2` (second return) which are `LaserData` protos.\n",
    "- `LaserData` contains `range_image_cartesian` (point coordinates as a 3D tensor: range, azimuth, elevation -> x,y,z) and `range_image_features` (intensity, elongation, etc.).\n",
    "- The Waymo SDK's `frame_utils.parse_range_image_and_camera_projection()` can convert range images to point clouds and project them to cameras.\n",
    "- **Why it's important:** Correctly parsing and converting LiDAR data into a usable format (like a list of [x,y,z] points) is fundamental.\n",
    "\n",
    "**b) Motion Distortion Compensation (Deskewing):**\n",
    "- **Concept:** LiDAR points are collected sequentially as the sensor spins and vehicle moves. This can cause skew in the point cloud. Deskewing corrects point positions using vehicle pose information.\n",
    "- **Waymo Data:** `Frame` contains `pose` (vehicle pose at `frame.timestamp_micros`). Individual points within a scan have their own timestamps (`point.time_offset_ns` relative to frame start) which can be used with vehicle motion to correct their positions.\n",
    "- **Why it's important:** Essential for accurate mapping, localization, and object detection.\n",
    "- **Techniques (Conceptual):**\n",
    "    - The SDK's `frame_utils.extract_point_cloud_from_range_image` handles deskewing if `frame.pose` is provided and point timestamps are used.\n",
    "    - For each point, interpolate vehicle pose to the precise point timestamp, then transform the point from sensor frame to vehicle frame, and then to world frame.\n",
    "\n",
    "**c) Outlier Detection/Removal:**\n",
    "- **Concept:** Removing erroneous points caused by sensor noise, reflections, or airborne particles (dust, rain).\n",
    "- **Why it's important:** Outliers can corrupt object detection and tracking.\n",
    "- **Techniques (Conceptual):**\n",
    "    - **Statistical Outlier Removal (SOR):** (e.g., using libraries like Open3D or PCL, or implementing a simpler version in NumPy/TF). Calculate mean distance to k-nearest neighbors; remove points if this distance is above a threshold.\n",
    "    - **Voxel Downsampling:** Can help reduce noise and density (`tf.raw_ops.VoxelGrid`).\n",
    "    - **Intensity/Elongation Filtering:** Points with very low intensity or unusual elongation might be noise.\n",
    "\n",
    "**d) Ground Plane Removal:**\n",
    "- **Concept:** Identifying and removing points belonging to the road surface.\n",
    "- **Why it's important:** Simplifies detection of objects like vehicles and pedestrians.\n",
    "- **Techniques (Conceptual):**\n",
    "    - **RANSAC Plane Fitting:** (e.g., using `skimage.measure.ransac` or implementing with TF/NumPy). Iteratively fit a plane model and remove inliers.\n",
    "    - **Height Thresholding:** If the sensor height is known and ground is relatively flat, remove points below a certain z-value in the vehicle coordinate frame.\n",
    "\n",
    "**e) Filtering by Range/Region of Interest (ROI):**\n",
    "- **Concept:** Keeping only points within a certain distance or a specific spatial region.\n",
    "- **Why it's important:** Reduces computational load and focuses analysis on relevant areas.\n",
    "- **Techniques:** Simple Euclidean distance filtering or bounding box checks in NumPy/TF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Code: Extracting, Deskewing, and Cleaning LiDAR Data\n",
    "\n",
    "# # Assume 'frame' is a parsed open_dataset.Frame object\n",
    "# # Assume 'lidar_name_to_process' is an open_dataset.LaserName.Name enum, e.g., open_dataset.LaserName.TOP\n",
    "\n",
    "# def get_lidar_data(frame, lidar_name_enum):\n",
    "#     for laser in frame.lasers:\n",
    "#         if laser.name == lidar_name_enum:\n",
    "#             return laser\n",
    "#     return None\n",
    "\n",
    "# # Conceptual: Process TOP LiDAR data\n",
    "# # top_lidar_proto = get_lidar_data(frame, open_dataset.LaserName.TOP)\n",
    "# # if top_lidar_proto:\n",
    "# #     # 1. Extract point cloud (SDK handles deskewing if pose is available)\n",
    "# #     # The SDK's frame_utils.parse_range_image_and_camera_projection can do this.\n",
    "# #     # Here's a more manual conceptual flow for understanding:\n",
    "# #     # Option 1: Use pre-calculated cartesian points if available and deskewing is handled by SDK implicitly\n",
    "# #     # if top_lidar_proto.ri_return1.range_image_cartesian.shape.dims:\n",
    "# #     #    cartesian_data_flat = tf.reshape(top_lidar_proto.ri_return1.range_image_cartesian, [-1, 3])\n",
    "# #     #    # This data is in sensor frame. Needs transformation using vehicle pose for full deskewing and world frame.\n",
    "# #     #    # The SDK utility `extract_point_cloud_from_range_image` is preferred as it handles point timestamps for deskewing.\n",
    "# #     #    print(f\"Extracted {cartesian_data_flat.shape[0]} raw LiDAR points conceptually.\")\n",
    "\n",
    "# #     # Using frame_utils for proper extraction and deskewing:\n",
    "# #     # (range_images, camera_projections, range_image_top_pose) = \\\n",
    "# #     #     frame_utils.parse_range_image_and_camera_projection(\n",
    "# #     #         top_lidar_proto, frame.context.camera_calibrations, frame.context.laser_calibrations,\n",
    "# #     #         frame.pose)\n",
    "\n",
    "# #     # points, cp_points = frame_utils.convert_range_image_to_point_cloud(\n",
    "# #     #     range_images[open_dataset.LaserName.TOP][0], # First return\n",
    "# #     #     frame.context.laser_calibrations, \n",
    "# #     #     range_image_top_pose, # This pose is used for deskewing\n",
    "# #     #     keep_polar_features=False\n",
    "# #     # )\n",
    "# #     # # `points` is a list of tensors [N, D], D usually 6 (x,y,z,intensity,elongation,timestamp_offset_ns)\n",
    "# #     # # For simplicity, let's assume points_xyz is extracted and concatenated for all returns if needed.\n",
    "# #     # if points and len(points) > 0:\n",
    "# #     #    points_xyz = points[0][:, :3].numpy() # Using first return, first 3 coords (x,y,z)\n",
    "# #     #    print(f\"Extracted and deskewed {points_xyz.shape[0]} LiDAR points using SDK utils.\")\n",
    "\n",
    "# #     #    # Visualize raw point cloud (conceptual - e.g. using Open3D or Matplotlib sample)\n",
    "# #     #    # fig = plt.figure(figsize=(10, 10))\n",
    "# #     #    # ax = fig.add_subplot(111, projection='3d')\n",
    "# #     #    # sample_indices = np.random.choice(points_xyz.shape[0], size=min(5000, points_xyz.shape[0]), replace=False)\n",
    "# #     #    # ax.scatter(points_xyz[sample_indices, 0], points_xyz[sample_indices, 1], points_xyz[sample_indices, 2], s=1)\n",
    "# #     #    # ax.set_title('Deskewed LiDAR Point Cloud (Sampled)')\n",
    "# #     #    # plt.show()\n",
    "\n",
    "# #     #    # 2. Outlier Removal (Conceptual SOR-like filter with NumPy)\n",
    "# #     #    # if points_xyz.shape[0] > 0:\n",
    "# #     #    #     # This is a very simplified version. Real SOR is more complex.\n",
    "# #     #    #     # For actual SOR, use libraries like Open3D: pcd.remove_statistical_outlier()\n",
    "# #     #    #     z_median = np.median(points_xyz[:, 2])\n",
    "# #     #    #     z_std = np.std(points_xyz[:, 2])\n",
    "# #     #    #     # Keep points within, e.g., 3 standard deviations from median Z (very basic heuristic)\n",
    "# #     #    #     points_filtered_z = points_xyz[(points_xyz[:, 2] > z_median - 3 * z_std) & (points_xyz[:, 2] < z_median + 3 * z_std)]\n",
    "# #     #    #     print(f\"Filtered points by Z-score heuristic: {points_filtered_z.shape[0]}\")\n",
    "\n",
    "# #     #    # 3. Ground Plane Removal (Conceptual Height Thresholding - assumes points are in vehicle frame)\n",
    "# #     #    #    # Ensure points are in a coordinate frame where Z is height relative to ground or vehicle.\n",
    "# #     #    #    # The SDK's `extract_point_cloud_from_range_image` returns points in vehicle frame.\n",
    "# #     #    #    ground_height_threshold = -1.5 # Example: remove points below -1.5m in vehicle frame\n",
    "# #     #    #    points_no_ground = points_filtered_z[points_filtered_z[:, 2] > ground_height_threshold]\n",
    "# #     #    #    print(f\"Points after conceptual ground removal: {points_no_ground.shape[0]}\")\n",
    "# # else:\n",
    "# #     print(\"TOP LiDAR data not found in frame.\")\n",
    "\n",
    "print(\"Conceptual: Extracted, deskewed (via SDK), and applied basic cleaning to LiDAR data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Synchronization and Multi-Sensor Consistency\n",
    "\n",
    "Waymo data is well-synchronized, but verifying and leveraging this is key for fusion.\n",
    "\n",
    "**a) Timestamp Verification:**\n",
    "- **Concept:** While `Frame` groups data by a common `timestamp_micros`, individual sensor readings within that frame might have slight offsets (e.g., camera exposure time, LiDAR scan duration).\n",
    "- **Waymo Data:** `CameraImage` has its own `pose_timestamp`. LiDAR points within a scan can have per-point timestamps (`time_offset_ns` from `frame.timestamp_micros`).\n",
    "- **Why it's important:** For precise fusion, understanding these minor timing differences can be crucial, especially at high speeds.\n",
    "- **Verification:** Check that `frame.images[i].pose_timestamp` is close to `frame.timestamp_micros`. The SDK handles LiDAR point deskewing using these fine-grained timestamps.\n",
    "\n",
    "**b) LiDAR-to-Image Projection:**\n",
    "- **Concept:** Projecting LiDAR points onto camera image planes to combine 3D structure with visual appearance.\n",
    "- **Why it's important:** Enables rich data association, validation of sensor calibration, and can be used for tasks like coloring point clouds or generating pseudo-LiDAR from images.\n",
    "- **Waymo SDK:** The `frame_utils.project_range_image_to_camera` or `frame_utils.project_points_to_image` (if you have point cloud in vehicle frame) can perform this projection. It uses camera intrinsics, extrinsics, and LiDAR point data.\n",
    "- **Techniques (Conceptual):**\n",
    "    1.  Get LiDAR points in the vehicle frame (deskewed).\n",
    "    2.  Get camera calibration (intrinsics, extrinsics relative to vehicle) for the target camera.\n",
    "    3.  Transform LiDAR points from vehicle frame to the camera frame using camera extrinsics.\n",
    "    4.  Project points onto the image plane using camera intrinsics.\n",
    "    5.  Filter points that are behind the camera or outside image boundaries.\n",
    "- **Visualization:** Overlay projected LiDAR points on the camera image. Correct alignment indicates good calibration and synchronization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Code: Projecting LiDAR points to a Camera Image\n",
    "\n",
    "# # Assume 'frame' is a parsed open_dataset.Frame object\n",
    "# # Assume 'points_vehicle_frame_np' is a NumPy array of deskewed LiDAR points [N,3] in vehicle frame\n",
    "# # Assume 'target_camera_name' is an open_dataset.CameraName.Name enum, e.g., open_dataset.CameraName.FRONT\n",
    "\n",
    "# # # 1. Get camera image and calibration\n",
    "# # cam_image_proto = None\n",
    "# # for img_proto in frame.images:\n",
    "# #     if img_proto.name == target_camera_name:\n",
    "# #         cam_image_proto = img_proto\n",
    "# #         break\n",
    "\n",
    "# # camera_calib = get_camera_calibration(frame, target_camera_name) # Using helper from earlier\n",
    "\n",
    "# # if cam_image_proto and camera_calib and 'points_vehicle_frame_np' in locals() and points_vehicle_frame_np.shape[0] > 0:\n",
    "# #     # Decode image for visualization\n",
    "# #     # image_tensor = tf.image.decode_jpeg(cam_image_proto.image)\n",
    "# #     # image_np = image_tensor.numpy()\n",
    "\n",
    "# #     # Get camera intrinsics (fx, fy, cx, cy) and extrinsics (vehicle_to_image transform)\n",
    "# #     # Intrinsics from camera_calib.intrinsic\n",
    "# #     # Extrinsics from camera_calib.extrinsic (this is sensor to vehicle, need vehicle to sensor)\n",
    "# #     # The SDK's projection utilities handle these transformations correctly.\n",
    "\n",
    "# #     # Using SDK utility for projection (conceptual, assumes points are in the format expected by SDK)\n",
    "# #     # For `project_points_to_image`, points need to be in vehicle frame, which `points_vehicle_frame_np` is.\n",
    "# #     # projected_points_tensor, valid_indices_tensor = frame_utils.project_points_to_image(\n",
    "# #     #     tf.constant(points_vehicle_frame_np, dtype=tf.float32),\n",
    "# #     #     target_camera_name,\n",
    "# #     #     camera_calib.intrinsic,\n",
    "# #     #     tf.linalg.inv(tf.constant(camera_calib.extrinsic.transform, dtype=tf.float32)), # Extrinsic is sensor to vehicle, need vehicle to sensor\n",
    "# #     #     frame.context.camera_calibrations[0].width, # Assuming all cameras have same width/height for simplicity here\n",
    "# #     #     frame.context.camera_calibrations[0].height\n",
    "# #     # )\n",
    "# #     # projected_points_np = projected_points_tensor.numpy()[valid_indices_tensor.numpy()]\n",
    "# #     # print(f\"Projected {projected_points_np.shape[0]} LiDAR points onto {open_dataset.CameraName.Name.Name(target_camera_name)} image.\")\n",
    "\n",
    "# #     # # Visualize (conceptual)\n",
    "# #     # # image_with_lidar_dots = image_np.copy()\n",
    "# #     # # for i in range(projected_points_np.shape[0]):\n",
    "# #     # #     x, y = int(projected_points_np[i, 0]), int(projected_points_np[i, 1])\n",
    "# #     # #     if 0 <= x < image_np.shape[1] and 0 <= y < image_np.shape[0]:\n",
    "# #     # #         cv2.circle(image_with_lidar_dots, (x,y), radius=2, color=(0,255,0), thickness=-1)\n",
    "\n",
    "# #     # # plt.figure(figsize=(12,8))\n",
    "# #     # # plt.imshow(image_with_lidar_dots)\n",
    "# #     # # plt.title(f\"LiDAR points projected on {open_dataset.CameraName.Name.Name(target_camera_name)} (Conceptual)\")\n",
    "# #     # # plt.show()\n",
    "# # else:\n",
    "# #     print(\"Missing data for LiDAR to image projection (points, image, or calibration).\")\n",
    "\n",
    "print(\"Conceptual: Projected LiDAR points to camera image using SDK utilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Handling Missing or Incomplete Data (Conceptual)\n",
    "\n",
    "While the Waymo dataset is comprehensive, understanding how to deal with potential missing data is crucial for robust pipelines.\n",
    "\n",
    "**Identifying Missing Data:**\n",
    "- **Sensor Level:** A `Frame` might be missing an entire `Laser` or `CameraImage` proto if a sensor was temporarily offline (rare in Waymo dataset but possible in general).\n",
    "- **Within Sensor Data:** Some LiDAR returns might be missing, or parts of an image could be unusable due to severe artifacts.\n",
    "- **Labels:** If using labels, some objects might not be labeled, or labels could be inaccurate.\n",
    "\n",
    "**Strategies (Conceptual):**\n",
    "- **Flagging:**\n",
    "    - The most straightforward approach: identify and flag frames or sensor data that are missing or deemed unusable.\n",
    "    - Downstream modules can then choose to ignore this data or adapt their behavior.\n",
    "    - Example: If a specific camera image is too dark, set a quality flag for it.\n",
    "- **Simple Imputation (Use with Caution):**\n",
    "    - **Temporal:** For slowly changing data (like vehicle pose if a GPS reading is briefly lost), one might interpolate from previous and next valid frames. Waymo `Frame` poses are generally complete.\n",
    "    - **Spatial:** For minor missing parts in LiDAR (e.g., a few missing scan lines), some applications might interpolate from neighboring points, but this can introduce artifacts.\n",
    "- **Zero-Padding/Masking for ML Models:**\n",
    "    - If feeding data into machine learning models, missing sensor inputs might be replaced with zeros or a special mask value, provided the model is trained to handle this.\n",
    "- **Checking for Presence:**\n",
    "    - Before accessing `frame.images[i]` or `frame.lasers[j]`, always check `len(frame.images)` and `len(frame.lasers)`.\n",
    "    - The Waymo SDK and TFRecord format usually ensure presence unless a segment is corrupted, but good practice to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conceptual Code: Checking for data presence and flagging\n",
    "\n",
    "# # Assume 'frame' is a parsed open_dataset.Frame object\n",
    "\n",
    "# # Check for presence of TOP LiDAR data\n",
    "# top_lidar_present = False\n",
    "# for laser in frame.lasers:\n",
    "#     if laser.name == open_dataset.LaserName.TOP:\n",
    "#         top_lidar_present = True\n",
    "#         # Further check if ri_return1 has points\n",
    "#         if not laser.ri_return1.range_image_cartesian.shape.dims:\n",
    "#             print(f\"Warning: TOP LiDAR present but range_image_cartesian is empty for frame {frame.timestamp_micros}\")\n",
    "#             top_lidar_present = False # Or set a quality flag\n",
    "#         break\n",
    "\n",
    "# if not top_lidar_present:\n",
    "#     print(f\"Data quality issue: TOP LiDAR data missing or empty for frame {frame.timestamp_micros}\")\n",
    "#     # Potentially skip this frame for LiDAR-dependent processing or use an alternative strategy\n",
    "\n",
    "# # Check for presence and basic quality of FRONT camera image\n",
    "# front_camera_ok = False\n",
    "# for cam_image in frame.images:\n",
    "#     if cam_image.name == open_dataset.CameraName.FRONT:\n",
    "#         if cam_image.image: # Check if image data bytes exist\n",
    "#             front_camera_ok = True\n",
    "#             # Conceptual: Add a basic brightness check (pseudo-code)\n",
    "#             # image_tensor = tf.image.decode_jpeg(cam_image.image)\n",
    "#             # if tf.reduce_mean(tf.image.rgb_to_grayscale(image_tensor)) < 30: # Example threshold\n",
    "#             #     print(f\"Warning: FRONT camera image for frame {frame.timestamp_micros} seems too dark.\")\n",
    "#             #     # front_camera_ok = False # Or set a specific quality flag\n",
    "#         else:\n",
    "#             print(f\"Warning: FRONT camera proto present but image data bytes are missing for frame {frame.timestamp_micros}\")\n",
    "#         break\n",
    "\n",
    "# if not front_camera_ok:\n",
    "#     print(f\"Data quality issue: FRONT camera data missing or problematic for frame {frame.timestamp_micros}\")\n",
    "\n",
    "print(\"Conceptual: Checked for data presence and applied simple quality flags.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "**Recap:**\n",
    "This tutorial provided a conceptual walkthrough of data cleaning and quality enhancement techniques for the Waymo Open Dataset. We covered:\n",
    "- Understanding the TFRecord format and `Frame` structure.\n",
    "- Accessing and processing camera data (decoding, undistortion, artifact considerations).\n",
    "- Accessing and processing LiDAR data (point cloud extraction, deskewing, outlier removal, ground plane removal).\n",
    "- The importance of sensor synchronization and cross-modal consistency (e.g., LiDAR-to-image projection).\n",
    "- Conceptual strategies for handling missing or incomplete data.\n",
    "\n",
    "**The Critical Role of Data Quality:**\n",
    "Working with large-scale AV datasets like Waymo's highlights the necessity of rigorous data cleaning. The performance of perception algorithms (object detection, segmentation, tracking) is directly tied to the quality of the input sensor data. While the Waymo dataset is already of high quality, these cleaning steps ensure robustness and help in understanding the data's nuances.\n",
    "\n",
    "**Pointers for Further Exploration:**\n",
    "- **Waymo Open Dataset Website & GitHub:** The official [Waymo Open Dataset GitHub repository](https://github.com/waymo-research/waymo-open-dataset) contains the SDK, tutorials, and more detailed documentation.\n",
    "- **TFRecord and TensorFlow `tf.data` API:** For efficient data loading and preprocessing, dive deeper into TensorFlow's data handling capabilities.\n",
    "- **Advanced Cleaning Techniques:** Explore more sophisticated algorithms for outlier removal (e.g., DBSCAN), noise filtering, and artifact correction, often found in libraries like Open3D, PCL (Point Cloud Library), and Scikit-image.\n",
    "- **Sensor Fusion Strategies:** Investigate advanced sensor fusion algorithms that can leverage the cleaned and synchronized data from multiple modalities.\n",
    "\n",
    "Remember that data cleaning is often an iterative process. Visualizing the data at each step is crucial to verify that the cleaning operations are having the desired effect and not inadvertently discarding useful information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12" # Example, will be environment specific
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
